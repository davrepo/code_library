{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Algebra Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step: 1\n",
    "- Compute the size of the graph, store it in a variable\n",
    "- Compute the reverse of the graph, using method reverse() and an array \"branching\" such that \"branching[i]\"\n",
    "    is the number of nodes linked to from node i. \n",
    "- Compute a list of dangling nodes\n",
    "- Compute the initial approx of the ranking vector corresponding to x0 of PageNoteNotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup\n",
    "G = nx.read_edgelist('p2p-Gnutella08-mod.txt', comments='#',    # NB! Gnutella.txt must be in the directory as specified\n",
    "                     create_using=nx.DiGraph(), \n",
    "                     delimiter='\\t', \n",
    "                     nodetype=int, \n",
    "                     encoding='utf-8')\n",
    "\n",
    "graph_size = G.size()\n",
    "G_reversed = nx.DiGraph.reverse(G)       # returns the reverse of the graph, i.e. all the edges are reversed\n",
    "\n",
    "# create an array \"branching\" such that branching[i] is the number of nodes linked to from node i, that is the out-degree of node i\n",
    "branching = np.zeros(graph_size, dtype=int)\n",
    "for idx, out_degree in G.out_degree():\n",
    "    branching[idx] = out_degree\n",
    "\n",
    "# find dangling nodes in the graph, that is nodes with out-degree 0\n",
    "dangling_nodes = np.where(branching == 0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Surfer implementation with networkx\n",
    "\n",
    "def random_walk(damping_factor, num_iterations, wantNumOfVisits = False):\n",
    "    # Select the initial random node \n",
    "    random_node = random.choice([i for i in range(G.number_of_nodes())])\n",
    "\n",
    "    # initialize the initial count for all nodes as zero\n",
    "    dict_counter = {}\n",
    "    for i in range(G.number_of_nodes()):\n",
    "        dict_counter[i] = 0\n",
    "\n",
    "    # Since we start with initial node, this node gets a count of 1 to start with\n",
    "    dict_counter[random_node] += 1\n",
    "\n",
    "    # Traversing through the neighrbors of random node\n",
    "    for i in range(num_iterations):     # num_iterations is the number of walks done in total\n",
    "        m = random.random()\n",
    "        list_of_neighbors = list(G.neighbors(random_node))\n",
    "        if random_node in dangling_nodes:     # if the node is a dangling node, then jump to a random node\n",
    "                random_node = random.choice([i for i in range(G.number_of_nodes())])\n",
    "                dict_counter[random_node] += 1\n",
    "        else:\n",
    "            if m < 1-damping_factor:  # probablity walk to random neighbor\n",
    "                random_node = random.choice(list_of_neighbors)   # select a random neighbor\n",
    "                dict_counter[random_node] += 1\n",
    "            else:       # jump to a random node, damping_factor% probability\n",
    "                random_node = random.choice([i for i in range(G.number_of_nodes())])\n",
    "                dict_counter[random_node] += 1\n",
    "    \n",
    "    # sort the dictionary by value in descending order\n",
    "    sorted_random_walk = sorted(dict_counter.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    if wantNumOfVisits:\n",
    "        return sorted_random_walk[:10]      # return the top 10 nodes with the most visits with their number of visits\n",
    "    else:\n",
    "        return list(map(operator.itemgetter(0), sorted_random_walk[:10]))     # return without the number of visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PageRank implementation with networkx (done naively)\n",
    "\n",
    "def pageRankNaive(damping_factor, num_iterations):\n",
    "    # compute mSx_k, this is done only once\n",
    "    mSx_k = np.full(graph_size, 1/graph_size) * damping_factor   \n",
    "\n",
    "    # initialize x_k0 as x_kx\n",
    "    x_kx = np.full(graph_size, 1/graph_size) \n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # compute Dx_k:\n",
    "        temp_sum_Dx = 0\n",
    "        for i in range(graph_size):\n",
    "            if i in dangling_nodes:\n",
    "                temp_sum_Dx += x_kx[i]/graph_size\n",
    "        Dx_k = np.full(graph_size, temp_sum_Dx)\n",
    "\n",
    "        # compute Ax_k by looping over the backlinks to i:\n",
    "        Ax_k = np.zeros(graph_size)\n",
    "        for i in G.nodes():\n",
    "            temp_sum_Ax = 0\n",
    "            for j in G_reversed.out_edges(i):\n",
    "                if branching[j[1]] != 0:\n",
    "                    temp_sum_Ax += x_kx[j[1]]/branching[j[1]]\n",
    "            Ax_k[i] = temp_sum_Ax\n",
    "\n",
    "        # putting all together\n",
    "        x_kx = (1-damping_factor)*Ax_k + (1-damping_factor)*Dx_k + mSx_k\n",
    "\n",
    "    return (-x_kx).argsort()[:10]    # return the top 10 nodes with the highest PageRank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0 13854 13853 13852 13851 13850 13849 13848 13855 13847]\n",
      "[127 266 249 123 352 367 424 264 251 427]\n",
      "[ 367  249  266  127  145  123  264  427  122 1317]\n",
      "[ 367  249  145  266  264  127  123  122 1317    5]\n",
      "[ 367  249  145  264  266  127  123  122 1317    5]\n",
      "[ 367  249  145  264  266  123  127  122 1317    5]\n",
      "as you can see from result below, it takes 4 iterations for the top 10 to stablise\n"
     ]
    }
   ],
   "source": [
    "# print a list of the 10 highest ranked nodes in the PageRank algorithm for Gnutella graph\n",
    "# Approx number of iterations of PageRank algorithm needed for the top 10 to stablise\n",
    "\n",
    "for i in range(6):\n",
    "    page_rank = pageRankNaive(0.15, i)  # 0.15 is the damping factor\n",
    "    print(page_rank)\n",
    "\n",
    "print(\"as you can see from result below, it takes 4 iterations for the top 10 to stablise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[367, 249, 266, 264, 145, 122, 127, 1317, 5, 123]\n",
      "it takes approximately 500_000 walks to get a result similar to the page rank algorithm\n"
     ]
    }
   ],
   "source": [
    "# print a list of the 10 most visited nodes in the Random Surfer simulation for Gnutella graph\n",
    "\n",
    "print(random_walk(0.15, 500_000, False))      # 0.15 is the damping factor, 500_000 walks, do not display number of visits for each node\n",
    "\n",
    "print(\"it takes approximately 500_000 walks to get a result similar to the page rank algorithm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5f0bfb910b5f89afee0c4e65874b118d019a6f05b0e3b9906c17a1424dc1b44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
