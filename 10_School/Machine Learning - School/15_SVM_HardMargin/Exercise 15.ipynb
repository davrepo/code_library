{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"50247bab5ef14ff3812df24eea074d40","deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":9,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["Status: âœ… Done"]},{"cell_type":"markdown","metadata":{"cell_id":"48013cbebaba49ea9850a1c91fdaae79","deepnote_cell_type":"markdown","tags":[]},"source":["## Exercise 15"]},{"cell_type":"markdown","metadata":{"cell_id":"7f2ced8e65c54fef952aca9234f3ebab","deepnote_cell_type":"markdown","tags":[]},"source":["---"]},{"cell_type":"markdown","metadata":{"cell_id":"469df3daac8a4ea7aa0d3fb8abab6dc2","deepnote_cell_type":"markdown","tags":[]},"source":["First, let me start by saying that `SVM` is not certainly the easiest topic to grasp, especially within a short span of time. In this exercise, we focus on **hard margin** SVM, which assumes that given feature space is linearly separable. In the next session, the focus will be on **soft margin** SVM which simply drops the assumption about the perfect separability. Either way, I can promise you that if you get hard margin SVM, then it is fairly simple to understand to the soft margin version.\n","\n","This exercise has therefore two **main learning objectives**:\n","- theory behind hard margin SVM: translation of an abstract idea (maximize the margin) to the actual mathematical model (primal and dual form)\n","- implementation of hard margin SVM: translation of the mathematical model to code\n","\n","Thus, also this notebook is split according to these two goals into two sections."]},{"cell_type":"markdown","metadata":{"cell_id":"6cfe8749a0244497a732b09822efa145","deepnote_cell_type":"markdown","tags":[]},"source":["### Theory behind hard margin SVM"]},{"cell_type":"markdown","metadata":{"cell_id":"b4caa1195ea34338a15270bc28a28e9d","deepnote_cell_type":"markdown","tags":[]},"source":["---"]},{"cell_type":"markdown","metadata":{"cell_id":"84913685ee2a431ba71ae69bfabff88a","deepnote_cell_type":"markdown","tags":[]},"source":["In this section, we go over the core concepts of `hard-margin SVM`. With this being said, for detailed explanation, you can refer to my detailed [note on SVM](https://ludekcizinsky.notion.site/Support-Vector-Machines-2db1ab88770641bc9815a7a9db45f72c) where I explain all the nitty gritty details. Alternatively, you can also watch this great [mit lecture](https://www.youtube.com/watch?v=_PwhiWxHK8o&t=35s) on the subject."]},{"cell_type":"markdown","metadata":{"cell_id":"c9b9bec6b5ea40f79af562b008c8e239","deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":18,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Recap: Hyperplanes"]},{"cell_type":"markdown","metadata":{"cell_id":"d031555e3a9448508ee363ca300353a4","deepnote_cell_type":"markdown","tags":[]},"source":["We first refresh the concept of hyperplanes, for this I prepared short [Geogebra note](https://www.geogebra.org/m/emrwvr3m)."]},{"cell_type":"markdown","metadata":{"cell_id":"c34dc5b6488f418a95414196e7043452","deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":39,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> New concept: high level idea behind SVM"]},{"cell_type":"markdown","metadata":{"cell_id":"948fb17055c54295a44461b67947a577","deepnote_cell_type":"markdown","tags":[]},"source":["Now, we are ready to jump into the high level idea of hard margin SVM. I prepared short [Geogebra note](https://www.geogebra.org/m/xyxnhqpy) on the subject. This note should help you understand the big picture of SVM, in my [notes](https://ludekcizinsky.notion.site/Support-Vector-Machines-2db1ab88770641bc9815a7a9db45f72c), I explain how we translate this abstract concept into mathematical model."]},{"cell_type":"markdown","metadata":{"cell_id":"db375ed9da704acaa1a8d9a664afd201","deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":57,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Model analysis: what are we optimising in hard margin SVM"]},{"cell_type":"markdown","metadata":{"cell_id":"b479568948034a68a83f7845be4e0b7c","deepnote_cell_type":"markdown","tags":[]},"source":["As explained in the high level idea, our objective is to maximize **width** subject to the correct classification of training samples. Therefore mathematically speaking ($\\text{width} = \\frac{2}{||w||}$):\n","\n","\n","$\n","\\underset{\\mathrm w, b_0} {\\arg\\max} \\frac{2}{||\\mathrm w||}\n","\\text{ subject to  } y(\\mathrm{w \\cdot x + b_0}) -1 \\ge 0\n","$\n","\n","However, the function $f(w) = \\frac{2}{||w||}$ is not easily differentiable, therefore, we instead use equivalent function $\\frac{1}{2}||w||^2$ which we, however, need to minimize:\n","\n","$\n","\\underset{\\mathrm w, b_0} {\\arg\\min} \\frac{1}{2} \\lVert  \\mathrm w \\rVert^2\n","\\text{ subject to  } y(\\mathrm{w \\cdot x + b_0}) -1 \\ge 0\n","$"]},{"cell_type":"markdown","metadata":{"cell_id":"f06ddc4f76814200ab5a0e39828c8339","deepnote_cell_type":"markdown","tags":[]},"source":["In words, we are trying to find model parameters $w,b_0$ such that the above function is minized subject to the given constraint. Just to be crystal clear, for instance in the case of the 2 features, $w = (b_1, b_2)$ and our model looks as follows:\n","\n","$\n","h(x) = b_2x + b_1y + b_0\n","$\n","\n","and then depending on the value of $h(x)$, we would classify the given input $x$. If $h(x) > 0$, then we classify for one class, else for the other. \n","\n","\n","Very importantly, we must first normalize the features, otherwise our model might overestimate importance of given feature. For instance, if we have one feature with range 0 - 100, and the other with range 0 - 1, then change by 1 in the **context of the first feature** (tiny change) means something very different than in the context of second feature (huge change going from min to max). In this case, our model would overestimate the importance of making a tiny step in first feature."]},{"cell_type":"markdown","metadata":{"cell_id":"ed5138fcc34a4fce8ddf082a0af2b423","color":"yellow","deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":15,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Section summary"]},{"cell_type":"markdown","metadata":{"cell_id":"0e017d893fb647579f2a2da8031f230a","deepnote_cell_type":"markdown","tags":[]},"source":["This section tried to give you a high level idea about working of hard margin SVM. You should be able to explain:\n","\n","- what is the core idea behind hard margin SVM as well as its assumption about linear separability\n","- what we are optimizing and why we need the constraint\n","\n","If you also read my detailed note, then you should be then able to explain how we actually derive the formula which we are trying to optimize."]},{"cell_type":"markdown","metadata":{"cell_id":"50bef81970b341b891db6ae676a5fc59","deepnote_cell_type":"markdown","tags":[]},"source":["### Implementation of hard margin SVM"]},{"cell_type":"markdown","metadata":{"cell_id":"310ce93541954095b7f52b98c8effc68","deepnote_cell_type":"markdown","tags":[]},"source":["---"]},{"cell_type":"markdown","metadata":{"cell_id":"08aafe2f8c024800912e714af143f5c2","deepnote_cell_type":"markdown","tags":[]},"source":["In this section, we will focus on implementing the hard margin SVM from scratch. I know, you have to understand math and programming at the same time, but if you manage go through this, I believe you will have a solid undrstanding of SVM. \n","\n","We will break the implementation into several steps. Let me first give you a small overview. In the course, your learned to rewrite the hard-margin SVM objective function as its **dual representation** (using Lagrangian multipliers), and maximize it:\n","\n","$\n","\\arg\\max_{\\alpha}  \\tilde{L}(\\alpha) = \\sum_i^n \\alpha_i - \\frac{1}{2} \\sum_i^n \\sum_j^n \\alpha_i \\alpha_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j})\n","$\n","\n","subject to $\\alpha \\geq 0$ and $\\sum_i^n \\alpha_i y_i = 0$. We know that any machine learning problem can be broken down essentialy to two stages:\n","\n","- **training**: we will find most optimal parameters $a$ using the above function $L$\n","- **prediction**: use the most optimal parameters $a$ to make prediction on given data\n","\n","In training, we need to:\n","- vectorize the loss function\n","- use `scipy's` optimization API to train our model\n","\n","In prediction, we need to:\n","- implement the predict function\n","- make prediction on the given data\n","\n","I will follow these steps in the subsequent sections."]},{"cell_type":"markdown","metadata":{"cell_id":"4e08d42b26474bc1895c07131cc80dc1","deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":37,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> training: loss function vectorisation"]},{"cell_type":"markdown","metadata":{"cell_id":"f900f5fd24684c1ea8fd9ca3ad352535","deepnote_cell_type":"markdown","tags":[]},"source":["We can vectorize the original loss function $L$ as:\n","\n","$\n","\\tilde{L}(\\alpha) = \\sum_{i} \\alpha_i - \\frac{1}{2} \\sum_{i,j} A_{i,j}, \\quad\n","\\text{where $A_{i,j}$ is the value at the $i$th row and $j$th column of $\\mathbf{A}$.}\n","$\n","\n","but what is $A$? We define $A$ as:\n","\n","$\n","\\mathbf{A}=\\boldsymbol{a}^{\\top} \\boldsymbol{\\alpha} \\circ \\mathbf{y} \\mathbf{y}^{\\top} \\circ \\boldsymbol{K}\n","$\n","\n","where:\n","- $K$ is an $N \\times N$ matrix resulted from the dot product between every two data points $\\mathbf{x}$, that is $K_{i, j}=\\mathbf{x}_{\\mathbf{i}} \\cdot \\mathbf{x}_{\\mathbf{j}}$\n","- $\\boldsymbol{\\alpha}$ is a $1 \\times N$ matrix in this equation, so $\\boldsymbol{\\alpha}^{\\top} \\boldsymbol{\\alpha}$ is an $N \\times N$-matrix\n","- $\\mathbf{y}$ is a $N \\times 1$ matrix in this equation, so $\\mathbf{y y}^{\\top}$ is an $N \\times N$-matrix\n","- The notation $\\circ$ in the equation stands for the element-wise product (Hadamard product)."]},{"cell_type":"markdown","metadata":{"cell_id":"001382e7aec54852b078bed166d8db85","deepnote_cell_type":"markdown","tags":[]},"source":["Let's get these done:\n","- First things first, we define the **interface of the model** and computation of the matrix $K$. You can see both of these in this [commit](https://github.com/ludekcizinsky/nano-learn/commit/f269d1ded6bc121230957c9d101942f4cdecbb9a?diff=unified).\n","\n","- Next, we vectorize the function, using the above formula for $A$, see this [commit](https://github.com/ludekcizinsky/nano-learn/commit/0d22458b7d9df71a3f48636bc13cb76ec94a16c4?diff=unified#diff-b014f61961234340adc4a28a71457d9b4060bfb6263ef98deac297caac9ea503R22)."]},{"cell_type":"markdown","metadata":{"cell_id":"e33ad7c8a51745d881a4b0873c462e38","deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":40,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> training: use scipy to find parameters a"]},{"cell_type":"markdown","metadata":{"cell_id":"ed8f7b2223e54e0ca83235184f3ce93b","deepnote_cell_type":"markdown","tags":[]},"source":["To use the QP solver [optimizer.minimize(..)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) function, we need to pass several items to it. Some of them are:\n","* a loss function $L$\n","* the **Jacobian matrix of the loss function** (Jacobian matrix is the partial derivatives of the loss function with respect to each element of $\\alpha$).\n","* **constraints**: the inequalities, their jacobian, the equalities, their jacobian\n","\n","Once we have, these, we can train the model, see the [commit](https://github.com/ludekcizinsky/nano-learn/commit/9f88cbc9d25eb374fb84ff4686e228182bc95847?diff=split)."]},{"cell_type":"markdown","metadata":{"cell_id":"4c294d98024f48cdb65f401373a5e494","deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":48,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> predict: implement and test the predict function"]},{"cell_type":"markdown","metadata":{"cell_id":"a99a3db65db94cd9a95907da62c13a15","deepnote_cell_type":"markdown","tags":[]},"source":["Now that we trained the model and have the $\\alpha$, we can use it to predict (classify) data points using:\n","\n","$\n","h(x) = \\sum_i^n \\alpha_i y_i( x_i \\cdot x )+ b \n","$\n","\n","where $b$:\n","\n","$\n","b = \\frac{1}{N_S}\\sum_{i \\in S}(y_i - \\sum_{j \\in S} \\alpha_jy_j(x_i.x_j)), \\quad \\text{where $S$ is the set of support vectors}\n","$"]},{"cell_type":"markdown","metadata":{"cell_id":"b0484bcfcdc94da98d1fa1e7602a784e","deepnote_cell_type":"markdown","tags":[]},"source":["Note that $x_i$ are our training samples, $x$ is the input for which we are predicting. Clearly, to make the prediction, we need to first compute $b$ which is essentialy a function of $a$ and our training samples. So technically, finding $b$ should be part of our training process, see this [commit](https://github.com/ludekcizinsky/nano-learn/commit/1cbd554614f8ba849ba5d02dda7cbac76e19bdfc). Finally, let's implement the prediction function and test whether our implentation is working, here is the corresponding [commit](https://github.com/ludekcizinsky/nano-learn/commit/5167926837b08cf2f5c19620d6651f653c7124fb). You should get following result:"]},{"cell_type":"markdown","metadata":{"cell_id":"d623a387bc1a466daf05acf794ea5d9f","deepnote_cell_type":"markdown","tags":[]},"source":["![](figures/svmclf.png)"]},{"cell_type":"markdown","metadata":{"cell_id":"7f37ff7fa5bd4f6f8ee37806ad26f937","color":"yellow","deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":15,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Section summary"]},{"cell_type":"markdown","metadata":{"cell_id":"2fb7d03d79304ca4833e728828121bba","deepnote_cell_type":"markdown","tags":[]},"source":["In this section, our focus was on implementing SVM from scratch using only numpy and scipy. This was certainly challenging for many reasons, but I believe if you manage to finish it, it should give you a really in depth understanding of hard margin svm. Finally, you can find the final implementation [here](https://github.com/ludekcizinsky/nano-learn/blob/main/nnlearn/svm/_svm.py)."]},{"cell_type":"markdown","metadata":{"cell_id":"a0c2f66804cd4db89ec7ffcf0842b4c8","deepnote_cell_type":"markdown","tags":[]},"source":["---"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fc95f382-8acc-4012-8b16-804e27725cfe' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"5f78f1615b7b47de84d29705e64f13a7","language_info":{"name":"python"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
