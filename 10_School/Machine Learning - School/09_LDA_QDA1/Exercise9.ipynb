{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"8aa9265b3bf6408aaa5135a43c35071d","deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":9,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["Status: âœ… done"]},{"cell_type":"markdown","metadata":{"cell_id":"4da3e14c8b78451c96ea05defa59b418","deepnote_cell_height":69.96875,"deepnote_cell_type":"markdown","tags":[]},"source":["## Exercise 9"]},{"cell_type":"markdown","metadata":{"cell_id":"1f31f14fab2742358d1878a7d037a84a","deepnote_cell_height":45.984375,"deepnote_cell_type":"markdown","tags":[]},"source":["---"]},{"cell_type":"markdown","metadata":{"cell_id":"8d23a9f2102e42ea85d3995d11d64199","deepnote_cell_height":74.765625,"deepnote_cell_type":"markdown","tags":[]},"source":["The weekend is over and we are back to business of machine learning, and specifically for this exercise we are going to focus on `generative models`. (by know you should what these are, if not, I suggest you revisit the previous exercise)"]},{"cell_type":"markdown","metadata":{"cell_id":"6675f6f41adb4553bc2180e3b8c0ae6a","deepnote_cell_height":46.375,"deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":7,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"e7419fc133ee4adaa28921e02dc9bee5","deepnote_cell_height":273.984375,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1876,"execution_start":1664288557824,"source_hash":"b4fa38b0","tags":[]},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from scipy import stats\n","from scipy.stats import multivariate_normal\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"markdown","metadata":{"cell_id":"29e2a06bff7049da9cd923a0df14548b","deepnote_cell_height":61.96875,"deepnote_cell_type":"markdown","tags":[]},"source":["### LDA & QDA overview"]},{"cell_type":"markdown","metadata":{"cell_id":"dd9811fb7caf4a4ea1ad082df982f659","deepnote_cell_height":45.984375,"deepnote_cell_type":"markdown","tags":[]},"source":["---"]},{"cell_type":"markdown","metadata":{"cell_id":"55fde32b4b0446edb1b07aee1cbae5f1","deepnote_cell_height":46.375,"deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":32,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Quick recap on generative models"]},{"cell_type":"markdown","metadata":{"cell_id":"17c1d98c17c643829d73c661342a7a76","deepnote_cell_height":514.171875,"deepnote_cell_type":"markdown","tags":[]},"source":["As discussed in the previous exercise, generative models work from a high level as follows:\n","\n","1. From the provided training data, try to estimate $p(C_k)$ `class prior` and then also $p(x|C_k)$ `class conditionals`. This corresponds to training part of the ML process.\n","2. Then use these estimates to compute `posterior probability` using `Bayes theorem` (part of prediction process):\n","    \n","$\n","p(C_k |x) = \\frac{p(x|C_k)p(C_k)}{p(x)}\n","$\n","\n","where $p(x) = \\sum_l^K p(x|C_l)p(C_l)$. Note that since this term is common to all classes, what really matters are the terms in numerator, i.e., class **priors** and **conditionals**. So in other words, we could only consider the `joint probability` $p(x, y)$ since:\n","\n","$\n","p(C_k, x) = p(x|C_k)p(C_k)\n","$\n","    \n","3. Finally, use some decision rule $d(x)$ to predict a class for given sample based on computed posterior probabilites class of the given sample $x$. As discussed in the last exercise, the most `optimal decision under 0-1 loss is to choose the class with highest posterior probability` - `Bayes Classifier`. So our decision is then:\n","    \n","$\n","d(x) = \\text{argmax } p(C_k|x)\n","$\n","\n","Note that you could also use:\n","\n","$\n","d(x) = \\log(\\text{argmax } p(C_k|x))\n","$\n","    \n","Since the argmax will remain the same."]},{"cell_type":"markdown","metadata":{"cell_id":"1063b1a4c19d40dbaefd017473771e8f","deepnote_cell_height":46.375,"deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":59,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Getting the decision function from Bayes theorem (advanced)"]},{"cell_type":"markdown","metadata":{"cell_id":"0f0ccb87852f47259ea79d2d98309d90","deepnote_cell_height":217.875,"deepnote_cell_type":"markdown","tags":[]},"source":["In this section, my goal will be to show how we get to the decision functions $d(x)$ for both classifiers. Note that this is quite cumbersome process so in the exam you will not be asked to reproduce this, but you should be able to explain the high level strategy how we get to the respective decision functions:\n","- we start with `Bayes` theorem\n","- we then plug in corresponding approximations for `class conditionals` and `priors`\n","\n","Therefore, the below derivation is only for those eager ones who really want to understand things in depth ðŸ¤” (but there are more important things to put your focus on)"]},{"cell_type":"markdown","metadata":{"cell_id":"fb10c248569e488c8037e535bc7f0db2","deepnote_cell_height":1966.671875,"deepnote_cell_type":"markdown","tags":[]},"source":["Let me start by writing down the Bayes theorem:\n","\n","$\n","p(C_k|x) = \\frac{P(x|C_k)p(C_k)}{p(x)}\n","$\n","\n","Since $p(x)$ serves as a normalisation and is common to all classes $K$, we can simply ignore it and reduce it to (note that we can only do this because of what I just mentioned):\n","\n","$\n","p(C_k|x) \\propto P(x|C_k)p(C_k)\n","$\n","\n","Since we want to choose a class with highest posterior probability, we can use log transformation on this formula and we will still get the same result:\n","\n","$\n","\\log (p(C_k|x)) \\propto \\log (P(x|C_k)p(C_k))\n","$\n","\n","We now continue by plugging our estimates, for our class prior $p(C_k)$ we have:\n","\n","$\n","p(C_k) = \\pi_k = \\frac{n_k}{n}\n","$\n","\n","And for our class conditional $p(x|C_k)$ we have:\n","\n","$\n","p(x|C_k) = \\frac{1}{2\\pi^{\\frac{m}{2}}|\\Sigma_k|^{-\\frac{1}{2}}}\\exp(-\\frac{1}{2}(x - \\mu_k)^T\\Sigma_k^{-1}(x - \\mu_k))\n","$\n","\n","So overall we have:\n","\n","$\n","\\log (p(C_k|x)) \\propto \\log(\\frac{1}{2\\pi^{\\frac{m}{2}}|\\Sigma_k|^{-\\frac{1}{2}}}\\exp(-\\frac{1}{2}(x - \\mu_k)^T\\Sigma_k^{-1}(x - \\mu_k))\\pi_k)\n","$\n","\n","We recall the rule for division in our log term, i.e.:\n","\n","$\n","\\log \\frac{x}{y} = \\log x - \\log y\n","$\n","\n","And similarly for product where we instead use plus. First, using the division rule we obtain:\n","\n","$\n","\\log{(p(C_k|x))} \\propto \\log(\\exp(-\\frac{1}{2}(x - \\mu_k)^T\\Sigma_k^{-1}(x - \\mu_k))\\pi_k) - \\log(2\\pi^{\\frac{m}{2}}|\\Sigma_k|^{-\\frac{1}{2}})\n","$\n","\n","And now using multiplication rule we can write:\n","\n","$\n","\\log(P(C_k|x)) \\propto -\\frac{1}{2}(x - \\mu_k)^T\\Sigma_k^{-1}(x - \\mu_k) + \\log(\\pi_k) + \\frac{1}{2}\\log(|\\Sigma_k|)- \\frac{m}{2}\\log(2\\pi)\n","$\n","\n","To make it nicer we multiply it by two:\n","\n","$\n","2\\log(P(C_k|x)) \\propto -(x - \\mu_k)^T\\Sigma_k^{-1}(x - \\mu_k) + 2\\log(\\pi_k) + \\log(|\\Sigma_k|)- m\\log(2\\pi)\n","$\n","\n","Notice that the last term does not depend on $k$, therefore we can drop it and obtain:\n","\n","$\n","2\\log(P(C_k|x)) \\propto -(x - \\mu_k)^T\\Sigma_k^{-1}(x - \\mu_k) + 2\\log(\\pi_k) + \\log(|\\Sigma_k|)\n","$\n","\n","We now focus on the expansion of the first term:\n","\n","$\n","-(x - \\mu_k)^T\\Sigma_k^{-1}(x - \\mu_k)\n","$\n","\n","We first use the following linear algebra rule:\n","\n","$\n","(A - B)^T = A^T - B^T\n","$\n","\n","Therefore, we get:\n","\n","$\n","-(x^T - \\mu_k^T)\\Sigma_k^{-1}(x - \\mu_k)\n","$\n","\n","Then we expand the left bracket as follows:\n","\n","$\n","-(x^T\\Sigma_k^{-1} - \\mu_k^T\\Sigma_k^{-1})(x - \\mu_k)\n","$\n","\n","Now we again expand the two brackets as:\n","\n","$\n","-(x^T\\Sigma_k^{-1}x - \\mu_k^T\\Sigma_k^{-1}x - x^T\\Sigma_k^{-1}\\mu_k + \\mu_k^T\\Sigma_k^{-1}\\mu_k)\n","$\n","\n","We then use another rule for working with matrices:\n","\n","$\n","(AB)^T = B^TA^T\n","$\n","\n","In addition, realize that $\\Sigma_k^{-1}$ is a **square symmetrical matrix**. This means if we take its transpose, we again obtain the exact same matrix. One more note, each of the terms above ends up being a real number, and we know that transpose of a real number yields again the exact same number.  Given all these assumptions, we can write the following:\n","\n","$\n","x^T\\Sigma_k^{-1}\\mu_k = (x^T\\Sigma_k^{-1}\\mu_k)^T = (\\Sigma_k^{-1}\\mu_k)^Tx =  \\mu_k^T\\Sigma_k^{-1}x\n","$\n","\n","Therefore, we can reduce the two middle terms into one as follows:\n","\n","$\n","-(x^T\\Sigma_k^{-1}x - 2\\mu_k^T\\Sigma_k^{-1}x + \\mu_k^T\\Sigma_k^{-1}\\mu_k)\n","$\n","\n","So overall we obtain:\n","\n","$\n","2\\log(P(C_k|x)) \\propto -(x^T\\Sigma_k^{-1}x - 2\\mu_k^T\\Sigma_k^{-1}x + \\mu_k^T\\Sigma_k^{-1}\\mu_k) + 2\\log(\\pi_k) + \\log(|\\Sigma_k|)\n","$\n","\n","We can then do the following substitution:\n","\n","- $a = -\\mu_k^T\\Sigma_k^{-1}\\mu_k + 2\\log(\\pi_k) + \\log(|\\Sigma_k|)$ (last three terms)\n","- $b = 2\\mu_k^T\\Sigma_k^{-1}$\n","- $c = -\\Sigma_k^{-1}$\n","\n","So we obtain:\n","\n","$\n","2\\log(P(C_k|x)) \\propto x^Tcx + bx + a = cx^Tx + bx + a\n","$\n","\n","Due to the first term, we have the **quadratic** discriminant. Notice the **similarity to LDA**, where the only difference is that we have $\\Sigma$ instead of $\\Sigma_k$. Therefore we drop the first quadratic term as it is no longer dependent on $k$. As such for LDA we can write:\n","\n","$\n","2\\log(P(C_k|x)) \\propto bx + a\n","$"]},{"cell_type":"markdown","metadata":{"cell_id":"f1d4a15456a548b084036832cf6edd00","deepnote_cell_height":150.984375,"deepnote_cell_type":"markdown","tags":[]},"source":["So this means we can conclude the following for each respective classifier:\n","\n","- `LDA`: $d(x) = \\text{argmax}_k \\text{ } bx + a$\n","- `QDA`: $d(x) = \\text{argmax}_k \\text{ } cx^Tx + bx + a$\n","\n","In simple words, you predict the class with the highest result value $r$ where $r \\propto P(C_k | x)$."]},{"cell_type":"markdown","metadata":{"cell_id":"0de2658a149742acb8b79d29c11d07e7","deepnote_cell_height":46.375,"deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":35,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Bonus: LDA and Mahalanobis distance"]},{"cell_type":"markdown","metadata":{"cell_id":"9f3838cb50b041d09b8bedad2b5e7c94","deepnote_cell_height":843.0625,"deepnote_cell_type":"markdown","tags":[]},"source":["So now we have arrived at the formula for $d(x)$ of `LDA`. If we rewrite the above derived formula in a more compact form, we get:\n","\n","$\n","\\text{argmax}_k \\text{ } d(C_k| x)_{\\mu_k, \\Sigma} = 2\\log \\pi_k - (x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k)\n","$\n","\n","This means that the decision essentially depends on two terms. The first term is simple, it is the log of prior of the given class. Recall that prior can be between 0 and 1, therefore:\n","- large prior yields **small negative number**\n","- small prior yields **large negative number**\n","\n","The second term can be interpretted as a **square of special kind of distance**. (I will explain in a bit what distance and why square) Since distance can not be negative, we will get some positive value miltiplied by negative sign. Therefore, the smaller this distance is, the better. Why? Let me give you an example.\n","\n","> We get **high** class prior = **log negative number**, we get **small** distance = **low negative number**, result must be also some **low negative** number. Ideally as close as possible to zero (best possible result). This is because we use **argmax**.\n","\n","Now you hopefully got the big picture. Time to explain the distance metric. It is called **Mahalanobis distance** and is defined as follows:\n","\n","$\n","d_m = \\sqrt{(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k)}\n","$\n","\n","Therefore, if we use square transformation we get:\n","\n","$\n","d_m^2 = (x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k)\n","$\n","\n","Plugging this back into our formula:\n","\n","$\n","\\text{argmax}_k \\text{ } d(C_k| x)_{\\mu_k, \\Sigma} = 2\\log \\pi_k - d_m^2\n","$\n","\n","So what is so special about `Mahalanobis distance` and why can not we simply use the `Eucliden distance`? Simple answer would be:\n","\n","> Eucliden distance measures **distance between two points**, whereas Mahalanobis distance measures **distance between a point and given distribution**.\n","\n","In other words, Mahalanobis distance takes into account variance of each feature as well as dependence of features (correlation). In practice, the distribution to which we are measuring the distance is the **class conditional gaussian distribution**. Therefore, the closer the given input $x$ is to the mean of the given gaussian, the better. This makes sense since we want to classify the given $x$ to the group (class) to which it looks most similar to in the feature space. (the similarity here is measure by Mahalanobis distance) If you want even more details, then I suggest you read this [article](https://www.machinelearningplus.com/statistics/mahalanobis-distance/)."]},{"cell_type":"markdown","metadata":{"cell_id":"c3ceaafbb2a04eae98066103f234138f","color":"yellow","deepnote_cell_height":46.375,"deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":15,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Section summary"]},{"cell_type":"markdown","metadata":{"cell_id":"b609f8ed886e4041873291c48a20d7a6","deepnote_cell_height":74.765625,"deepnote_cell_type":"markdown","tags":[]},"source":["This section was supposed to summarize core things about both LDA and QDA as well as show some more advanced things like Mahalanobis distance. In the below section, we will practice these theoretical concepts."]},{"cell_type":"markdown","metadata":{"cell_id":"3e69bde3820143cdae50c21eb72205c5","deepnote_cell_height":61.96875,"deepnote_cell_type":"markdown","tags":[]},"source":["### Deep dive into LDA and decision theory"]},{"cell_type":"markdown","metadata":{"cell_id":"5674eabcb33c4d7cb733a60e037e561a","deepnote_cell_height":45.984375,"deepnote_cell_type":"markdown","tags":[]},"source":["---"]},{"cell_type":"markdown","metadata":{"cell_id":"5aaacd2baa014e2a96944fc2e68a8f11","deepnote_cell_height":710.4375,"deepnote_cell_type":"markdown","tags":[]},"source":["Let's consider the following `population` defined by its class conditionals:\n","\n","$\n","\\begin{aligned}\n","p(x \\mid Y=\\text { black }) &=\\mathcal{N}(2,1) \\\\\n","p(x \\mid Y=\\text { red }) &=\\mathcal{N}(4,1) \\\\\n","p(x \\mid Y=\\text { blue }) &=\\mathcal{N}(7,1)\n","\\end{aligned}\n","$\n","\n","NB! `same variance for all classes`\n","\n","and the class priors $\\left(\\pi_{\\mathrm{black}}, \\pi_{\\mathrm{red}}, \\pi_{\\mathrm{blue}}\\right)=(0.6,0.1,0.3)$. Here is also a corresponding plot of unormalized posteriors $P(C_k | X)$ for each $k$:\n","\n","![population plot](images/img1.png)"]},{"cell_type":"markdown","metadata":{"cell_id":"422796591aa54d1ea171074c00213251","deepnote_cell_height":46.375,"deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":29,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Solving the problem in theory"]},{"cell_type":"markdown","metadata":{"cell_id":"9292e379cda847dbb4e7d57ab6ded6f2","deepnote_cell_height":583.8125,"deepnote_cell_type":"markdown","tags":[]},"source":["Before jumping into code, let's start with simple evaluation of the problem. First, how would the decision regions look like? We can draw these as follows:\n","\n","![Decision regions](images/img2.png)"]},{"cell_type":"markdown","metadata":{"cell_id":"c21265d9e0804be79fc1a8138d29fbf8","deepnote_cell_height":583.8125,"deepnote_cell_type":"markdown","tags":[]},"source":["Now, how many possible misclassifications can we make and what would be the corresponding probabilites? Since we have 3 classes, the confusion matrix would be 3 x 3. Using this notion we can visualize it as follows:\n","\n","![Misclassifications](images/img3.png)"]},{"cell_type":"markdown","metadata":{"cell_id":"2087ce1837f7407499a2a59ac0f52718","deepnote_cell_height":240.5,"deepnote_cell_type":"markdown","tags":[]},"source":["Finally, let's actually find out the exact boundaries for the decision regions. Since we have an access to the image, we can easily see that we are looking for value $x$ such that $P(C_{black}|x) = P(C_{red}|x)$ for the first boundary and similarly for the second one. To simplify things, we will only use function $g(x)$ which is proportional to the posterior probability:\n","\n","$\n","d(x) = \\text{argmax}_k \\text{ } g(x) = \\text{argmax}_k \\text{ } bx + a\n","$\n","\n","If we now substitute for the parameters $a, b$, we get: `Important Formula for Decision Boundary`\n","\n","$\n","g_{k}(x) = \\frac{\\mu_k}{\\sigma_k^{2}}x - \\frac{\\mu_k^{2}}{2\\sigma_k^{2}} + log(\\pi_k)\n","$"]},{"cell_type":"markdown","metadata":{"cell_id":"bec5d30bf10042c8ad40d596ed616920","deepnote_cell_height":632.15625,"deepnote_cell_type":"markdown","tags":[]},"source":["Therefore, for the first boundary, we can start by writing:\n","$\n","g_{black}(x) = g_{red}(x)\n","$\n","\n","This yields the following linear equations:\n","\n","$\n","\\begin{aligned}\n","g_{black}(x) \\approx 2x - 2.5 \\\\\n","g_{red}(x) \\approx 4x - 10.3\n","\\end{aligned}\n","$\n","\n","Then we solve for $x$ as follows:\n","\n","$\n","\\begin{aligned}\n","g_{black}(x) = g_{red}(x) \\\\\n","2x - 2.5 = 4x - 10.3 \\\\\n","-2x = - 7.8 \\\\\n","x = 3.9\n","\\end{aligned}\n","$\n","\n","For the second boundary, we can proceed similarly and write:\n","\n","$\n","\\begin{aligned}\n","g_{red}(x) \\approx 4x - 10.3 \\\\\n","g_{blue}(x) \\approx 7x - 25.7\n","\\end{aligned}\n","$\n","\n","And then again solve for $x$:\n","\n","$\n","\\begin{aligned}\n","g_{red}(x) = g_{blue}(x) \\\\\n","4x - 10.3 =  7x  - 25.7 \\\\\n","-3x = -15.4 \\\\\n","x = 5.1\n","\\end{aligned}\n","$\n","\n","Therefore we can conlude that the two boundaries are at $x_1 = 3.9$ and $x_2 = 5.1$."]},{"cell_type":"markdown","metadata":{"cell_id":"aef4dc39a4384b0dbc653bf6aad96318","deepnote_cell_height":46.375,"deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":31,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Solving the problem in practice"]},{"cell_type":"markdown","metadata":{"cell_id":"255817f74c9447189114b60761472c76","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["First, we want to define our `Bayes` classifier based on the provided information:"]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"bae02bd9d8ac4e2681da9182086c6969","deepnote_cell_height":1029.96875,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1664288559741,"source_hash":"b77cb556","tags":[]},"outputs":[],"source":["class BayesClassifier:\n","\n","    \"\"\"Bayes Classifier based on the provided information\n","    \"\"\"\n","\n","    def __init__(self):\n","        # -- class conditionals\n","        self.params = [(2, 1), (4, 1), (7, 1)]\n","\n","        # -- class priors\n","        self.priors = (.6, .3, .1)\n","    \n","    def predict_probs(self, x):\n","        \"\"\"Predict posteriors for given input\n","\n","        Attributes\n","        ----------\n","        x : 1d array\n","            Each element represents one possible input since we have one dimensional distribution\n","\n","        Returns\n","        -------\n","        result : 2d array\n","            n x m matrix where n is the number of inputs and k is the number of classes\n","        \n","        Todo\n","        ----\n","        I am converting back and forth list to arrays which is not so good for performance but it works \n","        for educatinal purposes. Feel free to rewrite this more efficiently.\n","        \"\"\"\n","\n","        # Result will be n, m matrix\n","        n, k = x.shape[0], 3\n","\n","        # Result for now just 2d list\n","        result = []\n","\n","        for i in range(n):\n","\n","            # Compute the likelihhods\n","            likelihoods = []\n","            xin = x[i]\n","            for j in range(k):\n","                likelihood = stats.norm.pdf(xin, self.params[j][0], self.params[j][1])*self.priors[j]\n","                likelihoods.append(likelihood)\n","            \n","            # Get posteriors\n","            px = sum(likelihoods)\n","            posteriors = list(np.array(likelihoods)/px)\n","\n","            # Save the result\n","            result.append(posteriors)\n","        \n","        return np.array(result)"]},{"cell_type":"markdown","metadata":{"cell_id":"80bc417c3b4e40ce98559d4fce8c0596","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["Let's test it out:"]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"4650a62c2e5e49898f41e782117f17ce","deepnote_cell_height":332.921875,"deepnote_cell_type":"code","deepnote_output_heights":[96.953125],"deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1664288559742,"source_hash":"85b5d724","tags":[]},"outputs":[{"data":{"text/plain":["array([[9.36620517e-01, 6.33789015e-02, 5.81743303e-07],\n","       [9.23659140e-01, 7.63399144e-02, 9.45859649e-07],\n","       [9.08306506e-01, 9.16919604e-02, 1.53353832e-06],\n","       [8.90233129e-01, 1.09764393e-01, 2.47806789e-06],\n","       [8.69110487e-01, 1.30885524e-01, 3.98870282e-06]])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Some data\n","x = np.arange(2, 8, .1)\n","\n","# Get the predictions for each input\n","bayes = BayesClassifier()\n","posteriors = bayes.predict_probs(x)\n","\n","# Show first 5 rows\n","posteriors[:5]"]},{"cell_type":"markdown","metadata":{"cell_id":"099fc5f0608d4e8b8c5a73b96e3e9a96","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["Let's also double check that each row sums to one:"]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"8a6331e04382445e9dda323ccb2e7996","deepnote_cell_height":169.734375,"deepnote_cell_type":"code","deepnote_output_heights":[77.765625],"deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1664288559746,"source_hash":"c646c40c","tags":[]},"outputs":[{"data":{"text/plain":["array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1.])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["posteriors.sum(axis=1)"]},{"cell_type":"markdown","metadata":{"cell_id":"fe00bcd7312c41718e4aaa4b10c4d480","deepnote_cell_height":561.421875,"deepnote_cell_type":"markdown","tags":[]},"source":["Now, back to the problem of getting the probabilities, let me just reinsert the image here so we know what we are up to:\n","\n","![misclassification](images/img3.png)\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"a743189a1de044e7919508c5b6782781","deepnote_cell_height":88.765625,"deepnote_cell_type":"markdown","tags":[]},"source":["We are going to solve the problem in three stages. We are going to get the misclassification probabilities highlighted by\n","\n","(1) The `green` color: **we predict black, but true is either red or blue**. "]},{"cell_type":"code","execution_count":23,"metadata":{"cell_id":"f6a9ebc6b3d14055b9ed045fb160bca6","deepnote_cell_height":291.96875,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":142,"execution_start":1664288559801,"source_hash":"d270a362","tags":[]},"outputs":[{"data":{"text/plain":["69.26808000994428"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Side note: in the posterior matrix\n","#   Each row = posteriors for the given sample\n","#   Columns represent posteriors for: Black (0), Red (1), Blue (2)\n","\n","# Stage 1\n","# -- Get the posteriors\n","x1 = np.arange(-3, 3.9, .01)\n","posteriors1 = bayes.predict_probs(x1)\n","\n","# Compute each respective error\n","true_red_predict_black = posteriors1[:, 1].sum()\n","true_blue_predict_black = posteriors1[:, 2].sum()\n","green_area = true_red_predict_black + true_blue_predict_black\n","green_area"]},{"cell_type":"markdown","metadata":{"cell_id":"e45259e2484347738f322bd6dc8078e7","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["(2) The `blue` color: **we predict red, but true is either black or blue**. "]},{"cell_type":"code","execution_count":24,"metadata":{"cell_id":"be3d84d7e3c34f2e8280aaf22c000937","deepnote_cell_height":291.96875,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1664288560005,"source_hash":"3c5f9827","tags":[]},"outputs":[{"data":{"text/plain":["15.620017797080765"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# Side note: in the posterior matrix\n","#   Each row = posteriors for the given sample\n","#   Columns represent posteriors for: Black (0), Red (1), Blue (2)\n","\n","# Stage 2\n","# -- Get the posteriors\n","x2 = np.arange(3.9, 5.1, .01)\n","posteriors2 = bayes.predict_probs(x2)\n","\n","# Compute each respective error\n","true_black_predict_red = posteriors2[:, 0].sum()\n","true_blue_predict_red = posteriors2[:, 2].sum()\n","blue_area = true_black_predict_red + true_blue_predict_red\n","blue_area"]},{"cell_type":"markdown","metadata":{"cell_id":"53aab123143846ca9a21d44764d1b8fe","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["(3) The `yellow` color: **we predict blue, but true is either black or red**. "]},{"cell_type":"code","execution_count":25,"metadata":{"cell_id":"57bdfd4debf14967af134dd3c2c00f5b","deepnote_cell_height":291.96875,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":147,"execution_start":1664288560051,"source_hash":"cb199dd4","tags":[]},"outputs":[{"data":{"text/plain":["80.49798357147995"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# Side note: in the posterior matrix\n","#   Each row = posteriors for the given sample\n","#   Columns represent posteriors for: Black (0), Red (1), Blue (2)\n","\n","# Stage 3\n","# -- Get the posteriors\n","x3 = np.arange(5.1, 11, .01)\n","posteriors3 = bayes.predict_probs(x3)\n","\n","# Compute each respective error\n","true_black_predict_blue = posteriors3[:, 0].sum()\n","true_red_predict_blue = posteriors3[:, 1].sum()\n","yellow_area = true_black_predict_blue + true_red_predict_blue\n","yellow_area"]},{"cell_type":"markdown","metadata":{"cell_id":"7210f69d69714647bdffc877804eb7eb","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["Let's double check that we got correct results:"]},{"cell_type":"code","execution_count":9,"metadata":{"cell_id":"ef88e475fb15444e8a22fcac413f30f1","deepnote_cell_height":112.15625,"deepnote_cell_type":"code","deepnote_output_heights":[20.1875],"deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1664288560203,"source_hash":"3ecfe882","tags":[]},"outputs":[{"data":{"text/plain":["True"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["true_red_predict_blue > true_black_predict_blue"]},{"cell_type":"markdown","metadata":{"cell_id":"d8843b9d7b3c4417991c9666ed8a2729","deepnote_cell_height":97.15625,"deepnote_cell_type":"markdown","tags":[]},"source":["The reason why this should be true is that if we look at the graph, then the red line has way higher posteriors than the black line so naturally, if we sum all over these, black sum should be lower. Finally, we can also obtain the total irreducible bayes error rate as:"]},{"cell_type":"code","execution_count":10,"metadata":{"cell_id":"540891e545904998abb3c2b6d0624cbd","deepnote_cell_height":166.15625,"deepnote_cell_type":"code","deepnote_output_heights":[20.1875],"deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1664288560259,"source_hash":"18d6af46","tags":[]},"outputs":[{"data":{"text/plain":["0.11813291527036071"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["total_error = green_area + blue_area + yellow_area\n","n1, n2, n3 = len(x1), len(x2), len(x3)\n","bayes_error = total_error/(n1 + n2 + n3)\n","bayes_error"]},{"cell_type":"markdown","metadata":{"cell_id":"b18a07c7d0464d4dbca6eb6a0bc19c3c","deepnote_cell_height":74.921875,"deepnote_cell_type":"markdown","tags":[]},"source":["Note that this number is just an approximation since the input $x$ is not continuous (the step size is $0.01$) so depending on that you might get a slightly different value."]},{"cell_type":"markdown","metadata":{"cell_id":"aa6b4a91213c4c7baa80c4226f34486c","deepnote_cell_height":46.375,"deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":64,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Estimating parameters and comparing this to the Bayes classifier"]},{"cell_type":"markdown","metadata":{"cell_id":"22f345fc548b4963b640ea791ce7cc4b","deepnote_cell_height":74.765625,"deepnote_cell_type":"markdown","tags":[]},"source":["Now, we are going to sample from the defined population `training` and `test` data, train a new `LDA` model and see how it performs against the above `Bayes` classifier. Let's start with sampling:"]},{"cell_type":"code","execution_count":11,"metadata":{"cell_id":"742427aac7c2434792ffbfaaebb1b8a9","deepnote_cell_height":507.984375,"deepnote_cell_type":"code","deepnote_output_heights":[192.921875],"deepnote_to_be_reexecuted":false,"execution_millis":86,"execution_start":1664288560260,"source_hash":"a74a9215","tags":[]},"outputs":[],"source":["# Define size of of sample\n","N = 1400\n","\n","# Save the sampled data to these\n","x = []\n","y = []\n","\n","for _ in range(N):\n","    p = np.random.random()\n","    if p < .6:\n","        p = np.random.random()\n","        x.append(stats.norm.ppf(p, 2, 1))\n","        y.append(0)\n","    elif p < .9:\n","        p = np.random.random()\n","        x.append(stats.norm.ppf(p, 4, 1))\n","        y.append(1)\n","    else:\n","        p = np.random.random()\n","        x.append(stats.norm.ppf(p, 7, 1))\n","        y.append(2)\n","\n","# Save it as pandas\n","d = {'x': x, 'y': y}\n","data = pd.DataFrame(data=d)"]},{"cell_type":"code","execution_count":12,"metadata":{"cell_id":"b5cef8fa668f40b58a216ed4957a36e8","deepnote_cell_height":389.9375,"deepnote_cell_type":"code","deepnote_table_loading":false,"deepnote_table_state":{"filters":[],"pageIndex":0,"pageSize":25,"sortBy":[]},"deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1664288560374,"source_hash":"41313cfa","tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.242353</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.086894</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.208785</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.130867</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.662471</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          x  y\n","0  2.242353  0\n","1  1.086894  0\n","2  4.208785  1\n","3  4.130867  1\n","4  4.662471  1"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["data.head()"]},{"cell_type":"markdown","metadata":{"cell_id":"438c92c1b1df4b55ae4462deadecffe9","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["Let's do train test split:"]},{"cell_type":"code","execution_count":13,"metadata":{"cell_id":"fe034b4433c640f49481d08f4d7ff84c","deepnote_cell_height":75.96875,"deepnote_cell_type":"code","deepnote_output_heights":[20.1875],"deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1664288560375,"source_hash":"a30f84a4","tags":[]},"outputs":[],"source":["train, test = train_test_split(data, test_size=0.3)"]},{"cell_type":"markdown","metadata":{"cell_id":"32db9a75292f4d0dbf3a10f120ce0f1d","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["Now, we can fit our `LDA` model:"]},{"cell_type":"code","execution_count":14,"metadata":{"cell_id":"d9759ba61f404edc8b17ffca0aa962fc","deepnote_cell_height":75.96875,"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1664288560376,"source_hash":"2e1be3e8","tags":[]},"outputs":[],"source":["m1 = LinearDiscriminantAnalysis().fit(train[['x']], train['y'])"]},{"cell_type":"markdown","metadata":{"cell_id":"0c0863c2322d4527a0e82bcf3ac9cde9","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["If you wanted to do this manually, you need to estimate class means, and variance:"]},{"cell_type":"code","execution_count":15,"metadata":{"cell_id":"650bb4a573464d5783fcb9ddf69de0dc","deepnote_cell_height":183.984375,"deepnote_cell_type":"code","deepnote_output_heights":[20.1875],"deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1664288560420,"source_hash":"e7164ad0","tags":[]},"outputs":[],"source":["# Class means\n","mean_black = train[train['y'] == 0]['x'].mean()\n","mean_red = train[train['y'] == 1]['x'].mean()\n","mean_blue = train[train['y'] == 2]['x'].mean()\n","\n","# Variance\n","common_var = train['x'].var()"]},{"cell_type":"markdown","metadata":{"cell_id":"3aa0e35ca6d845b4a5f9bacc7170af7c","deepnote_cell_height":97.15625,"deepnote_cell_type":"markdown","tags":[]},"source":["Then to get predictions for new data, you would write the exact same code as I wrote for the `bayes classifier`. Feel free to implement your own `LDA`, but I am going to use `sklearn` for this since the main focus is on the interpretation of the results. There, let's now see how our model works on test data:"]},{"cell_type":"code","execution_count":26,"metadata":{"cell_id":"8e5c1c0cb8144b81ad020e80d0b1aecb","deepnote_cell_height":93.984375,"deepnote_cell_type":"code","deepnote_output_heights":[58.5625],"deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1664288560421,"source_hash":"8ff17168","tags":[]},"outputs":[{"data":{"text/plain":["array([[225,  26,   0],\n","       [ 35,  88,   2],\n","       [  0,   7,  37]], dtype=int64)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["yhat = m1.predict(test[['x']])\n","cm = confusion_matrix(test['y'], yhat)\n","cm"]},{"cell_type":"markdown","metadata":{"cell_id":"f18abd10a5774683a404fa2365a3ae64","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["Not that bad... ðŸ˜Œ Let's now divide the misclassifications by the total number of misclassifications:"]},{"cell_type":"code","execution_count":17,"metadata":{"cell_id":"a6824616521c4b448617804a81380ab4","deepnote_cell_height":186.53125,"deepnote_cell_type":"code","deepnote_output_heights":[58.5625],"deepnote_to_be_reexecuted":false,"execution_millis":9,"execution_start":1664288560422,"source_hash":"9daffd5d","tags":[]},"outputs":[{"data":{"text/plain":["array([[3.21428571, 0.37142857, 0.        ],\n","       [0.5       , 1.25714286, 0.02857143],\n","       [0.        , 0.1       , 0.52857143]])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["misclassifications_total = cm.sum() - np.diagonal(cm).sum()\n","cm_norm = cm/misclassifications_total\n","cm_norm"]},{"cell_type":"markdown","metadata":{"cell_id":"3a2df29ad4f743adb2c29b9387b9a8d4","deepnote_cell_height":74.765625,"deepnote_cell_type":"markdown","tags":[]},"source":["First, ignore the digaonal values since these do not mean anything. What we are interested however are the non-digaonal values. For instance, we see that the biggest misclassification rate has `true is red, but predicted black`:"]},{"cell_type":"code","execution_count":18,"metadata":{"cell_id":"c6c6dace675c48d0a6a8a2e66b51dae3","deepnote_cell_height":112.15625,"deepnote_cell_type":"code","deepnote_output_heights":[20.1875],"deepnote_to_be_reexecuted":false,"execution_millis":7,"execution_start":1664288560430,"source_hash":"848338e0","tags":[]},"outputs":[{"data":{"text/plain":["0.5"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["cm_norm[1, 0]"]},{"cell_type":"markdown","metadata":{"cell_id":"409661cbbf3d4edaae42c458725d97ee","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["Let's see what was the rate in `Bayes` classifier:"]},{"cell_type":"code","execution_count":19,"metadata":{"cell_id":"b7788e0e7fd44d88bac43812c769158f","deepnote_cell_height":112.15625,"deepnote_cell_type":"code","deepnote_output_heights":[20.1875],"deepnote_to_be_reexecuted":false,"execution_millis":9,"execution_start":1664288560436,"source_hash":"53a890b4","tags":[]},"outputs":[{"data":{"text/plain":["0.4184978033242"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["true_red_predict_black/total_error"]},{"cell_type":"markdown","metadata":{"cell_id":"0b62530ea2084f73b496cc05a6d5a66d","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["Relatively similar. How about `true is black, predicted blue`?"]},{"cell_type":"code","execution_count":20,"metadata":{"cell_id":"bfd80d78c9da4ab3aff97da4e8d6f322","deepnote_cell_height":112.15625,"deepnote_cell_type":"code","deepnote_output_heights":[20.1875],"deepnote_to_be_reexecuted":false,"execution_millis":5,"execution_start":1664288560451,"source_hash":"66eb1dc5","tags":[]},"outputs":[{"data":{"text/plain":["0.006099309430870282"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["true_black_predict_blue/total_error "]},{"cell_type":"markdown","metadata":{"cell_id":"668d8df8b99c46d185d1ffb9c63365ca","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":[" Finally, let's check the estimate of bayes error rate:"]},{"cell_type":"code","execution_count":21,"metadata":{"cell_id":"1e912443ec1143dfa282857656b37b6e","deepnote_cell_height":112.15625,"deepnote_cell_type":"code","deepnote_output_heights":[20.1875],"deepnote_to_be_reexecuted":false,"execution_millis":7,"execution_start":1664288560455,"source_hash":"48b52804","tags":[]},"outputs":[{"data":{"text/plain":["0.16666666666666663"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["1 - accuracy_score(test['y'], yhat)"]},{"cell_type":"markdown","metadata":{"cell_id":"930f422a0c5d4fb494ccf8d45483a0f3","deepnote_cell_height":164.34375,"deepnote_cell_type":"markdown","tags":[]},"source":["Given that our model is supposed to estimate `Bayes`, we can conclude that it is doing that relatively well.\n","One last thought experiment, what if we trained the model on a sample with balanced classes, would we get even closer to estimating `bayes classifier`? As I have mentioned in the previous exercise, ideally we want our model to see as many as possible different inputs for each respective class. Therefore, if we would change the priors in training to do this, I believe it would have a positive impact on our model's performance as long as we then re-adjust the posteriors according to the actual priors. (See previous exercise where there is a whole section dedicated to it)"]},{"cell_type":"markdown","metadata":{"cell_id":"0532b225cb6f427c94a05345de715437","color":"yellow","deepnote_cell_height":46.375,"deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":15,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Section summary"]},{"cell_type":"markdown","metadata":{"cell_id":"cac940deef5c4478810ce1baca131958","deepnote_cell_height":141.953125,"deepnote_cell_type":"markdown","tags":[]},"source":["In this section, we have focused on working with `LDA` in connection to the decision theory. We first saw how we would make predictions if we know the whole population's parameters. Then we switched perspective and focused on how estimating these parameters and then comparing our model based on these parameters to the `Bayes` classifier which is based on the population's parameters. This showed that despite having access to all information, there will always be some space for error which comes from the uncertainity when predicting given class with highest posterior probability."]},{"cell_type":"markdown","metadata":{"cell_id":"554ae6be8db34e0ca88f67cf39729cb4","deepnote_cell_height":61.96875,"deepnote_cell_type":"markdown","tags":[]},"source":["### LDA vs QDA"]},{"cell_type":"markdown","metadata":{"cell_id":"3155c953f3e949a78f0fef864b4decc7","deepnote_cell_height":45.984375,"deepnote_cell_type":"markdown","tags":[]},"source":["---"]},{"cell_type":"markdown","metadata":{"cell_id":"6da8b505bc59435fae5f9af2881e72c9","deepnote_cell_height":46.375,"deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":6,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Theory"]},{"cell_type":"markdown","metadata":{"cell_id":"7cda7a081b3d4c94ab9109f0bddeaa1b","deepnote_cell_height":192.34375,"deepnote_cell_type":"markdown","tags":[]},"source":["The core difference between `LDA` and `QDA` is the following assumption:\n","\n","> QDA assumes each class has a specific mean as well as variance\n","\n","You can check this assumption by plotting the data. Alternatively, you can fit both models and compare their generalization error. In general, `QDA` is more complex, which you might need or not. This also means that you need to estimate more parameters for `QDA` and as such you need more data. To conclude, whether to use `QDA` or `LDA` depends on the data at hand: is it aligned with either model's assumption? how much data do you actually have?"]},{"cell_type":"markdown","metadata":{"cell_id":"50f8bf00e40c4d428c4a6ddc8224c6a3","deepnote_cell_height":46.375,"deepnote_cell_type":"text-cell-callout","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":8,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["> Practice"]},{"cell_type":"markdown","metadata":{"cell_id":"9168e7e6eac24215a9acace7fdb97c22","deepnote_cell_height":52.375,"deepnote_cell_type":"markdown","tags":[]},"source":["Let's try this in practice - we will QDA and see its generalization error."]},{"cell_type":"code","execution_count":22,"metadata":{"cell_id":"f8bfbb55f5bc4e8d9e7c2209c9dbc1ef","deepnote_cell_height":130.15625,"deepnote_cell_type":"code","deepnote_output_heights":[20.1875,20.1875],"deepnote_to_be_reexecuted":false,"execution_millis":1,"execution_start":1664288560509,"source_hash":"f30603e0","tags":[]},"outputs":[{"data":{"text/plain":["0.17142857142857137"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["m2 = QuadraticDiscriminantAnalysis().fit(train[['x']], train['y'])\n","1 - accuracy_score(test['y'], m2.predict(test[['x']]))"]},{"cell_type":"markdown","metadata":{"cell_id":"892807f164d3452aa22a5ac73b112b2b","deepnote_cell_height":74.765625,"deepnote_cell_type":"markdown","tags":[]},"source":["Almos the same performance, therefore the conclusion is for this particualr case that `LDA` is good enough since we also saw we got very close to the `Bayes` classifier. Session concluded! ðŸ¥³"]},{"cell_type":"markdown","metadata":{"cell_id":"1a7e74c7aee947ad98dabc2e0fd34e14","deepnote_cell_height":45.984375,"deepnote_cell_type":"markdown","tags":[]},"source":["---"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown","tags":[]},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=dac56272-d26a-4895-8563-759c36ec57fa' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"7c374968e83c43498db022ecfd3b0c5d","deepnote_persisted_session":{"createdAt":"2022-09-27T14:46:46.136Z"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
