{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Second Year Project\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the weekly assignment of week 1. The assignments are split up per lecture. You can upload your solutions on LearnIt.\n",
    "\n",
    "# Lecture 1. What are words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Regular Expressions\n",
    "For this section, it might be handy to use the website https://regex101.com/ to test your solutions.\n",
    "\n",
    "- a) Write a regular expression (regex or pattern) that matches any of the following words: `cat`, `sat`, `mat`.\n",
    "<br>\n",
    "(Bonus: What is a possible long solution? Can you find a shorter solution? *hint*: match characters instead of words)\n",
    "- b) Write a regular expression that matches numbers, e.g. 12, 1,000, 39.95\n",
    "- c) Expand the previous solution to match Danish price indications, e.g., `1,000 kr` or `39.95 DKK` or `19.95`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) \n",
    "- Long solution <br>\n",
    "  `\\b(?:cat|sat|mat)\\b`\n",
    "- Short solution <br>\n",
    "  `[acmst]{3}`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) `\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) `\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\s*(kr|DKK)?\\b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "(Adapted notebook from S. Riedel, UCL & Facebook: https://github.com/uclnlp/stat-nlp-book).\n",
    "\n",
    "In Python, a simple way to tokenize a text is via the `split` method that divides a text wherever a particular substring is found. In the code below this pattern is simply the whitespace character, and this seems like a reasonable starting point for an English tokenization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " \"thinkin'\",\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan.\\nWhy',\n",
       " \"doesn't\",\n",
       " 'he',\n",
       " 'quit?']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Mr. Bob Dobolina is thinkin' of a master plan.\" + \\\n",
    "       \"\\nWhy doesn't he quit?\"\n",
    "text.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make more fine-grained decisions, we will focus on using regular expressions for tokenization in this assignment. This can be done by either:\n",
    "1. Defining the character sequence patterns at which to split.\n",
    "2. Specifying patters that define what constitutes a token. \n",
    "\n",
    "In the code below we use a simple pattern `\\s` that matches **any whitespace** to define where to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " \"thinkin'\",\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan.',\n",
       " 'Why',\n",
       " \"doesn't\",\n",
       " 'he',\n",
       " 'quit?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "gap = re.compile('\\s')\n",
    "gap.split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One **shortcoming** of this tokenization is its treatment of punctuation because it considers `plan.` as a token whereas ideally we would prefer `plan` and `.` to be distinct tokens. It might be easier to address this problem if we define what a token is, instead of what constitutes a gap. Below we have defined tokens as sequences of alphanumeric characters and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr',\n",
       " '.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " 'thinkin',\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan',\n",
       " '.',\n",
       " 'Why',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'he',\n",
       " 'quit',\n",
       " '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = re.compile('\\w+|[.?:]')\n",
    "token.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still isn't perfect as `Mr.` is split into two tokens, but it should be a single token. Moreover, we have actually lost an apostrophe. Both are fixed below, although we now fail to break up the contraction `doesn't`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " \"thinkin'\",\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan',\n",
       " '.',\n",
       " 'Why',\n",
       " \"doesn't\",\n",
       " 'he',\n",
       " 'quit',\n",
       " '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = re.compile('Mr.|[\\w\\']+|[.?]')\n",
    "tokens = token.findall(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we have an input text and apply the tokenizer (described previously) on the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Curiouser\", 'and', 'curiouser', \"'\", 'cried', 'Alice', 'she', 'was', 'so', 'much', 'surprised', 'that', 'for', 'the', 'moment', 'she', 'quite', 'forgot', 'how', 'to', 'speak', 'good', 'English', \"'now\", \"I'm\", 'opening', 'out', 'like', 'the', 'largest', 'telescope', 'that', 'ever', 'was', 'Good', 'bye', 'feet', \"'\", 'for', 'when', 'she', 'looked', 'down', 'at', 'her', 'feet', 'they', 'seemed', 'to', 'be', 'almost', 'out', 'of', 'sight', 'they', 'were', 'getting', 'so', 'far', 'off', '.', \"'Oh\", 'my', 'poor', 'little', 'feet', 'I', 'wonder', 'who', 'will', 'put', 'on', 'your', 'shoes', 'and', 'stockings', 'for', 'you', 'now', 'dears', '?', \"I'm\", 'sure', 'I', \"shan't\", 'be', 'able', 'I', 'shall', 'be', 'a', 'great', 'deal', 'too', 'far', 'off', 'to', 'trouble', 'myself', 'about', 'you', 'you', 'must', 'manage', 'the', 'best', 'way', 'you', 'can', 'but', 'I', 'must', 'be', 'kind', 'to', 'them', \"'\", 'thought', 'Alice', \"'or\", 'perhaps', 'they', \"won't\", 'walk', 'the', 'way', 'I', 'want', 'to', 'go', 'Let', 'me', 'see', \"I'll\", 'give', 'them', 'a', 'new', 'pair', 'of', 'boots', 'every', 'Christmas', '.', '.', '.', \"'\"]\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"'Curiouser and curiouser!' cried Alice (she was so much surprised, that for the moment she quite\n",
    "forgot how to speak good English); 'now I'm opening out like the largest telescope that ever was! Good-bye,\n",
    "feet!' (for when she looked down at her feet, they seemed to be almost out of sight, they were getting so far\n",
    "off). 'Oh, my poor little feet, I wonder who will put on your shoes and stockings for you now, dears? I'm sure I\n",
    "shan't be able! I shall be a great deal too far off to trouble myself about you: you must manage the best\n",
    "way you can; —but I must be kind to them,' thought Alice, 'or perhaps they won't walk the way I want to go!\n",
    "Let me see: I'll give them a new pair of boots every Christmas...'\n",
    "\"\"\"\n",
    "\n",
    "token = re.compile('Mr.|[\\w\\']+|[.?]')\n",
    "tokens = token.findall(text)\n",
    "print(tokens) # print(tokens[:10])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "* a) The tokenizer clearly makes a few mistakes. Where?\n",
    "\n",
    "* b) Write a tokenizer to tokenize the text correctly by your own definition.\n",
    "\n",
    "* c) Should one separate `'m`, `'ll`, `n't`, possessives, and other forms of contractions from the word? Implement a tokenizer that separates these, and attaches the `'` to the latter part of the contraction.\n",
    "\n",
    "* d) Should elipsis (...) be considered as three `.`s or one `...`? Design a regular expression for both solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) \n",
    "- That the sign `'` is not considered as a separate token and instead it is connected with a word.\n",
    "- We missed `!`, `,`, `:`, `;`, `()`, `-`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) \n",
    "```\n",
    "\\b\\w+(?:'\\w+)?\\b|[.,!?;:()—'\\\"…]\n",
    "```\n",
    "\n",
    "- `\\b\\w+(?:'\\w+)?\\b` matches whole words, including those with a trailing apostrophe followed by more word characters (for contractions and possessive endings), ensuring they are kept as a single token. The `\\b` on both ends ensures we are matching whole words.\n",
    "- `[.,!?;:()—'\\\"…]` matches individual punctuation marks and special characters, ensuring they are treated as separate tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'Curiouser', 'and', 'curiouser', '!', \"'\", 'cried', 'Alice', '(', 'she', 'was', 'so', 'much', 'surprised', ',', 'that', 'for', 'the', 'moment', 'she', 'quite', 'forgot', 'how', 'to', 'speak', 'good', 'English', ')', ';', \"'\", 'now', \"I'm\", 'opening', 'out', 'like', 'the', 'largest', 'telescope', 'that', 'ever', 'was', '!', 'Good', 'bye', ',', 'feet', '!', \"'\", '(', 'for', 'when', 'she', 'looked', 'down', 'at', 'her', 'feet', ',', 'they', 'seemed', 'to', 'be', 'almost', 'out', 'of', 'sight', ',', 'they', 'were', 'getting', 'so', 'far', 'off', ')', '.', \"'\", 'Oh', ',', 'my', 'poor', 'little', 'feet', ',', 'I', 'wonder', 'who', 'will', 'put', 'on', 'your', 'shoes', 'and', 'stockings', 'for', 'you', 'now', ',', 'dears', '?', \"I'm\", 'sure', 'I', \"shan't\", 'be', 'able', '!', 'I', 'shall', 'be', 'a', 'great', 'deal', 'too', 'far', 'off', 'to', 'trouble', 'myself', 'about', 'you', ':', 'you', 'must', 'manage', 'the', 'best', 'way', 'you', 'can', ';', '—', 'but', 'I', 'must', 'be', 'kind', 'to', 'them', ',', \"'\", 'thought', 'Alice', ',', \"'\", 'or', 'perhaps', 'they', \"won't\", 'walk', 'the', 'way', 'I', 'want', 'to', 'go', '!', 'Let', 'me', 'see', ':', \"I'll\", 'give', 'them', 'a', 'new', 'pair', 'of', 'boots', 'every', 'Christmas', '.', '.', '.', \"'\"]\n",
      "174\n"
     ]
    }
   ],
   "source": [
    "tokenizer_regex = r\"\\b\\w+(?:'\\w+)?\\b|[.,!?;:()—'\\\"…]\"\n",
    "\n",
    "tokens = re.findall(tokenizer_regex, text)\n",
    "\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Separating contractions and possessive forms into their base words and suffixes can be beneficial for certain natural language processing tasks, as it helps in understanding the syntactic and semantic structures of sentences. For example, transforming \"I'm\" into \"I\" and \"'m\" or \"Alice's\" into \"Alice\" and \"'s\" can make it easier to analyze the grammatical roles of words in sentences.\n",
    "\n",
    "- `\\b\\w+\\b(?='[smtldve]\\b)` matches words that are followed by common contractions (such as 's, 'm, 't, 'll, 've) without including the apostrophe part in the match. It uses a positive lookahead to check for the contraction pattern without consuming it.\n",
    "- `[smtldve]\\b|'n'\\b|'re\\b|'ve\\b|'ll\\b|'d\\b|'s\\b|'m\\b|'t\\b` specifically matches the contraction parts, including the apostrophe.\n",
    "- `\\b\\w+\\b` matches standalone words.\n",
    "- `[.,!?;:()—'\\\"…]` matches individual punctuation marks and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'Curiouser', 'and', 'curiouser', '!', \"'\", 'cried', 'Alice', '(', 'she', 'was', 'so', 'much', 'surprised', ',', 'that', 'for', 'the', 'moment', 'she', 'quite', 'forgot', 'how', 'to', 'speak', 'good', 'English', ')', ';', \"'\", 'now', 'I', \"'m\", 'opening', 'out', 'like', 'the', 'largest', 'telescope', 'that', 'ever', 'was', '!', 'Good', 'bye', ',', 'feet', '!', \"'\", '(', 'for', 'when', 'she', 'looked', 'down', 'at', 'her', 'feet', ',', 'they', 'seemed', 'to', 'be', 'almost', 'out', 'of', 'sight', ',', 'they', 'were', 'getting', 'so', 'far', 'off', ')', '.', \"'\", 'Oh', ',', 'my', 'poor', 'little', 'feet', ',', 'I', 'wonder', 'who', 'will', 'put', 'on', 'your', 'shoes', 'and', 'stockings', 'for', 'you', 'now', ',', 'dears', '?', 'I', \"'m\", 'sure', 'I', 'shan', \"'t\", 'be', 'able', '!', 'I', 'shall', 'be', 'a', 'great', 'deal', 'too', 'far', 'off', 'to', 'trouble', 'myself', 'about', 'you', ':', 'you', 'must', 'manage', 'the', 'best', 'way', 'you', 'can', ';', '—', 'but', 'I', 'must', 'be', 'kind', 'to', 'them', ',', \"'\", 'thought', 'Alice', ',', \"'\", 'or', 'perhaps', 'they', 'won', \"'t\", 'walk', 'the', 'way', 'I', 'want', 'to', 'go', '!', 'Let', 'me', 'see', ':', 'I', \"'ll\", 'give', 'them', 'a', 'new', 'pair', 'of', 'boots', 'every', 'Christmas', '.', '.', '.', \"'\"]\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer regex that separates contractions and possessive forms\n",
    "tokenizer_regex = r\"\\b\\w+\\b(?='[smtldve]\\b)|'n'\\b|'re\\b|'ve\\b|'ll\\b|'d\\b|'s\\b|'m\\b|'t\\b|\\b\\w+\\b|[.,!?;:()—'\\\"…]\"\n",
    "\n",
    "tokens = re.findall(tokenizer_regex, text)\n",
    "\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) It depends on the use case. In some contexts, it might be preferable to consider an ellipsis as a single token (...), representing a pause or omission. In other cases, one might want to treat each period in an ellipsis as a separate token (.), especially if you're analyzing punctuation usage or performing syntactic parsing where the precise number of punctuation marks is relevant.\n",
    "\n",
    "Treating Ellipsis as One Token\n",
    "- `[\\w']+` matches words including those with apostrophes.\n",
    "- `[...]` specifically matches an ellipsis as a single token.\n",
    "- `[.,!?;:()—'\\\"…]` matches individual punctuation marks and special characters, excluding the ellipsis since it's already matched as a single token.\n",
    "\n",
    "Treating Each Period in an Ellipsis Separately\n",
    "- the regex is the same as before but without a specific pattern for `[...]`. This means it doesn't explicitly differentiate between ellipses and single periods, treating them all as individual punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating Ellipsis as One Token\n",
    "tokenizer_regex_ellipsis_single = r\"\\b\\w+\\b|[\\w']+|[...]|[.,!?;:()—'\\\"…]\"\n",
    "\n",
    "# Treating Each Period in an Ellipsis Separately\n",
    "tokenizer_regex_ellipsis_separate = r\"\\b\\w+\\b|[\\w']+|[.,!?;:()—'\\\"…]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Twitter Tokenization\n",
    "As you might imagine, tokenizing tweets differs from standard tokenization. There are 'rules' on what specific elements of a tweet might be (mentions, hashtags, links), and how they are tokenized. The goal of this exercise is not to create a bullet-proof Twitter tokenizer but to understand tokenization in a different domain.\n",
    "\n",
    "In the next exercises, we will focus on the following tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"@robv New vids coming tomorrow #excited_as_a_child, can't w8!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['robv', 'New', 'vids', 'coming', 'tomorrow', 'excited_as_a_child', 'can', 't', 'w8']\n"
     ]
    }
   ],
   "source": [
    "token = re.compile('[\\w]+')\n",
    "tokens = token.findall(tweet)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- a) What is the correct tokenization of the tweet above according to you?\n",
    "- b) Try your tokenizer from the previous exercise (Question 2). Which cases are going wrong? Rewrite your tokenizer such that it handles the above tweet correctly.\n",
    "- c) How will your tokenizer handle emojis?\n",
    "- d) Think of at least one example where your tokenizer (from b) will behave incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) \n",
    "- handle correctly user handles\n",
    "- handle correctly hash tags as separate tokens <br>\n",
    "Correct tokenization of the above tweet would be:\n",
    "\n",
    "```\n",
    "[@robv, New, vids, coming, tomorrow, #excited_as_a_child, can, 't, w8, !, !]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Previous regex is not capable of handling correctly user and hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['robv', 'New', 'vids', 'coming', 'tomorrow', 'excited_as_a_child', ',', 'can', \"'t\", 'w8', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.findall(tokenizer_regex, tweet)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix: \n",
    "- `@\\w+` to match usernames. This pattern captures the @ symbol followed by one or more word characters.\n",
    "- `#\\w+` to match hashtags. This pattern captures the # symbol followed by one or more word characters.\n",
    "- The rest of the pattern is similar to the previous version but has been slightly adjusted to ensure it can still match contractions, punctuation, and words correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@robv', 'New', 'vids', 'coming', 'tomorrow', '#excited_as_a_child', ',', 'can', \"'t\", 'w8', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# Fix\n",
    "tokenizer_regex_tweet = r\"@\\w+|#\\w+|\\b\\w+\\b(?='[smtldve]n?\\b)|'n'\\b|'re\\b|'ve\\b|'ll\\b|'d\\b|'s\\b|'m\\b|'t\\b|\\b\\w+\\b|[.,!?;:()—'\\\"…]\"\n",
    "tokens = re.findall(tokenizer_regex_tweet, tweet)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Emojis are not handled correctly. Emojis are Unicode characters that fall outside the ranges typically matched by \\w (which matches word characters like letters and digits) and the specific punctuation marks listed. To handle emojis in text tokenization, I would need to include a pattern that matches Unicode emoji characters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) example: \"Excited for the launch @midnight!!! 🚀 #new_beginnings... Wait, isn't it @midnight's show? 😕\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Excited', 'for', 'the', 'launch', '@midnight', '!', '!', '!', '#new_beginnings', '.', '.', '.', 'Wait', ',', 'isn', \"'t\", 'it', '@midnight', \"'s\", 'show', '?']\n"
     ]
    }
   ],
   "source": [
    "tweet_2 = \"Excited for the launch @midnight!!! 🚀 #new_beginnings... Wait, isn't it @midnight's show? 😕\"\n",
    "tokens_2 = re.findall(tokenizer_regex_tweet, tweet_2)\n",
    "print(tokens_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation is not a trivial task either.\n",
    "\n",
    "First, make sure you understand the following sentence segmentation code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_segment(match_regex, tokens):\n",
    "    \"\"\"\n",
    "    Splits a sequence of tokens into sentences, splitting wherever the given matching regular expression\n",
    "    matches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens      the input sequence as list of strings (each item is a ``word'')\n",
    "    match_regex the regular expression that defines at which token to split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a list of token lists, where each inner list represents a sentence.\n",
    "\n",
    "    >>> tokens = ['the','man','eats','.','She', 'sleeps', '.']\n",
    "    >>> sentence_segment(re.compile('\\.'), tokens)\n",
    "    [['the', 'man', 'eats', '.'], ['She', 'sleeps', '.']]\n",
    "    \"\"\"\n",
    "    sentences = [[]]\n",
    "    for tok in tokens:\n",
    "        sentences[-1].append(tok)\n",
    "        if match_regex.match(tok):\n",
    "            sentences.append([])\n",
    "            \n",
    "    if sentences[-1] == []:\n",
    "        del sentences[-1]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, there is a variable `text` containing a small text and a regular expression-based segmenter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch', 'is', 'the', 'longest', 'official', 'one', 'word', 'placename', 'in', 'U', '.']\n",
      "['K', '.']\n",
      "[\"Isn't\", 'that', 'weird', '?', 'I', 'mean', 'someone', 'took', 'the', 'effort', 'to', 'really', 'make', 'this', 'name', 'as', 'complicated', 'as', 'possible', 'huh', '?', 'Of', 'course', 'U', '.']\n",
      "['S', '.']\n",
      "['A', '.']\n",
      "['also', 'has', 'its', 'own', 'record', 'in', 'the', 'longest', 'name', 'albeit', 'a', 'bit', 'shorter', '...']\n",
      "['This', 'record', 'belongs', 'to', 'the', 'place', 'called', 'Chargoggagoggmanchauggagoggchaubunagungamaugg', '.']\n",
      "[\"There's\", 'so', 'many', 'wonderful', 'little', 'details', 'one', 'can', 'find', 'out', 'while', 'browsing', 'http', 'www', '.']\n",
      "['wikipedia', '.']\n",
      "['org', 'during', 'their', 'Ph', '.']\n",
      "['D', '.']\n",
      "['or', 'an', 'M', '.']\n",
      "['Sc', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch is the longest official one-word placename in U.K. Isn't that weird? I mean, someone took the effort to really make this name as complicated as possible, huh?! Of course, U.S.A. also has its own record in the longest name, albeit a bit shorter... This record belongs to the place called Chargoggagoggmanchauggagoggchaubunagungamaugg. There's so many wonderful little details one can find out while browsing http://www.wikipedia.org during their Ph.D. or an M.Sc.\n",
    "\"\"\"\n",
    "\n",
    "token = re.compile('Mr.|[\\w\\']+|[.?]+')\n",
    "\n",
    "tokens = token.findall(text)\n",
    "sentences = sentence_segment(re.compile('\\.'), tokens)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- a) Improve the segmenter so that it segments the text in the way you think is correct.\n",
    "- b) How could you deal with all URLs effectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) and b)\n",
    "- The current `sentence_segment` function should not only consider periods (.) as sentence delimiters, which is insufficient for the given text that includes question marks, exclamation points, and other sentence-ending punctuation.\n",
    "- First, adjust the tokenizer regex to better capture words, contractions, and a broader range of punctuation marks, including URLs to avoid splitting them into multiple tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch', 'is', 'the', 'longest', 'official', 'one', 'word', 'placename', 'in', 'U.', 'K', '.']\n",
      "[\"Isn't\", 'that', 'weird', '?']\n",
      "['I', 'mean', 'someone', 'took', 'the', 'effort', 'to', 'really', 'make', 'this', 'name', 'as', 'complicated', 'as', 'possible', 'huh', '?!']\n",
      "['Of', 'course', 'U.S.', 'A', '.']\n",
      "['also', 'has', 'its', 'own', 'record', 'in', 'the', 'longest', 'name', 'albeit', 'a', 'bit', 'shorter', '...']\n",
      "['This', 'record', 'belongs', 'to', 'the', 'place', 'called', 'Chargoggagoggmanchauggagoggchaubunagungamaugg', '.']\n",
      "[\"There's\", 'so', 'many', 'wonderful', 'little', 'details', 'one', 'can', 'find', 'out', 'while', 'browsing', 'http://www.wikipedia.org', 'during', 'their', 'Ph', '.']\n",
      "['D', '.']\n",
      "['or', 'an', 'M.', 'Sc', '.']\n"
     ]
    }
   ],
   "source": [
    "# Updated tokenizer regex to include URLs, and capture sentence-ending punctuation as separate tokens\n",
    "token_regex = re.compile(r'https?://\\S+|\\b(?:[A-Z]+\\.)+\\b|\\b[A-Za-z]+\\.[A-Za-z]+\\.\\b|\\w+\\'\\w+|\\w+|[.!?]+')\n",
    "\n",
    "tokens = token_regex.findall(text)\n",
    "\n",
    "def improved_sentence_segment(tokens):\n",
    "    \"\"\"\n",
    "    Splits a sequence of tokens into sentences, splitting at sentence-ending punctuation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: List of strings, where each string is a token.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of lists, where each inner list represents a sentence.\n",
    "    \"\"\"\n",
    "    sentences = [[]]\n",
    "    for tok in tokens:\n",
    "        sentences[-1].append(tok)\n",
    "        if re.match('[.!?]', tok):  # Adjusted to match any sentence-ending punctuation\n",
    "            sentences.append([])\n",
    "\n",
    "    if not sentences[-1]:\n",
    "        del sentences[-1]\n",
    "    return sentences\n",
    "\n",
    "# Segment into sentences\n",
    "sentences = improved_sentence_segment(tokens)\n",
    "\n",
    "# Print each sentence\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Tokenization competition\n",
    "\n",
    "Tokenization of social media can be more challenging. We provide a small development set for you to experiment with, which you can find in `week1/tok.dev.txt`. The file is a tab-separated file, where the first column contains the input text, and the second column the gold tokenization (as decided by an annotator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [line.strip().split('\\t') for line in open('tok.dev.txt', encoding=\"utf-8\")]\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a test file with the same format, but the gold annotation is missing. This can be found in `week1/tok.test.txt`. You are supposed to develop your tokenizer based on the development data, and then apply your tokenizer on the test data. You can hand in the predictions on the test data on LearnIt in the same slot as the rest of the assignment. We will use F1 score for evaluation.\n",
    "\n",
    "Make sure that the file you hand in:\n",
    "- Uses exactly the same format as the dev data (`input\\<tab\\>output`), where the input and output contain the same characters (except for placement of whitespaces). \n",
    "- Has your ITU username as the name of the file: i.e. `robv.txt`.\n",
    "\n",
    "We have provided an evaluation script for your convenience, it return F1 score, recall, and precision. It also prints out all sentences where your model made an error (indicating the error in red if supported by your terminal), and checks whether your output is in the right format. It can be found in `week1/tok_eval.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Language (correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spelling correction\n",
    "\n",
    "Below is an implementation of the Levenshtein distance. It uses a some efficiency tricks, and it is not important you understand every line of this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]\n",
    "\n",
    "print(levenshteinDistance('this', 'that'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide you with an English word list from [Aspell](http://aspell.net/) in `aspell-en-dict.txt`. It can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brower browser\n",
      "False\n",
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Load wordlist (one word per line)\n",
    "en_dict = set([word.strip() for word in open('aspell-en-dict.txt', encoding=\"utf-8\").readlines()])\n",
    "    \n",
    "# Example usage\n",
    "typo = 'brower'\n",
    "correction = 'browser'\n",
    "print(typo, correction)\n",
    "print(typo in en_dict)\n",
    "print(correction in en_dict)\n",
    "print(levenshteinDistance(typo, correction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Implement a (naive) spelling correction system that finds the word in the word list with the smallest minimum edit distance for a word that contains a misspelling. \n",
    "* b) There could be multiple words with the smallest minimum edit distance for some typos, what are supplementary methods to re-rank these? (mention at least 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) \n",
    "- Function `naiveSpellingCorrection()` iterates through each word in the provided dictionary, calculating the Levenshtein distance to the misspelled word. It keeps track of the word with the smallest distance seen so far.\n",
    "- This implementation is \"naive\" because it does not incorporate any optimizations like early stopping for words that are too long or too short compared to the misspelled word, nor does it use more sophisticated methods like trie structures for more efficient lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled word: brower\n",
      "Correction: brewer (Edit distance: 1)\n"
     ]
    }
   ],
   "source": [
    "def naiveSpellingCorrection(misspelled_word, word_list):\n",
    "    closest_word = None\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    for word in word_list:\n",
    "        distance = levenshteinDistance(misspelled_word, word)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_word = word\n",
    "            \n",
    "    return closest_word, min_distance\n",
    "\n",
    "typo = 'brower'\n",
    "correction, distance = naiveSpellingCorrection(typo, en_dict)\n",
    "print(f\"Misspelled word: {typo}\")\n",
    "print(f\"Correction: {correction} (Edit distance: {distance})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\n",
    "- When multiple candidate corrections for a typo have the same smallest minimum edit distance, supplementary methods can be employed to re-rank these candidates and select the most appropriate correction. Here are two methods for re-ranking:\n",
    "\n",
    "1. Word Frequency \n",
    "   \n",
    "2. Contextual Fit (using N-gram)\n",
    "- Description: The context in which a word appears can significantly influence its likelihood of being the correct correction. Using n-gram models can help assess the fit of each candidate correction within the context of the surrounding words.\n",
    "- Implementation: For each candidate correction, calculate the probability of the word given its context (the surrounding words) using an n-gram model. The word that results in the highest contextual probability is chosen as the best fit.\n",
    "\n",
    "Other Methods:\n",
    "\n",
    "3. Phonetic Similarity: For words with the same edit distance, the one that sounds more similar to the typo could be preferred. Algorithms like Soundex or the Metaphone family (Double Metaphone, Metaphone 3) can be used to compare the phonetic similarity between the typo and candidate corrections.\n",
    "   \n",
    "4. Semantic Similarity: Especially useful in context-aware spelling correction, leveraging word embeddings (e.g., Word2Vec, GloVe) can help identify the candidate word that is semantically closest to the expected word in the given context.\n",
    "   \n",
    "5. Part-of-Speech Matching: If the part-of-speech (POS) for the misspelled word can be inferred from the context, candidates can be re-ranked based on their POS match. This requires POS tagging of the context and the candidate words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Analysis of spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide you with a list of 100 typos and their corrections from the [GitHub Typo Corpus](https://aclanthology.org/2020.lrec-1.835/) in `typos.txt`. It can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "browers browser\n"
     ]
    }
   ],
   "source": [
    "# Load github typo corpus misspellings\n",
    "typos = []\n",
    "corrections = []\n",
    "for line in open('typos.txt', encoding=\"utf-8\"):\n",
    "    tok = line.strip().split('\\t')\n",
    "    typos.append(tok[0])\n",
    "    corrections.append(tok[1])\n",
    "    \n",
    "# Example usage\n",
    "print(typos[0], corrections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Evaluate the spelling correction system you implemented in the previous assignment with accuracy. How many of the words did it correct right?\n",
    "* b) Now evaluate the errors, can you identify some common causes (i.e. trends) in the mistakes of your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.00%\n",
      "Corrected 48 out of 100 typos.\n"
     ]
    }
   ],
   "source": [
    "# Load typos and corrections\n",
    "typos_path = './typos.txt'\n",
    "typos = []\n",
    "corrections = []\n",
    "\n",
    "with open(typos_path, 'r', encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        tok = line.strip().split('\\t')\n",
    "        typos.append(tok[0])\n",
    "        corrections.append(tok[1])\n",
    "\n",
    "# Evaluate the spelling correction system\n",
    "corrected_count = 0\n",
    "for typo, expected in zip(typos, corrections):\n",
    "    correction, _ = naiveSpellingCorrection(typo, en_dict)\n",
    "    if correction == expected:\n",
    "        corrected_count += 1\n",
    "\n",
    "accuracy = corrected_count / len(typos)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Corrected {corrected_count} out of {len(typos)} typos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typo: browers, Expected: browser, Correction: rowers\n",
      "Typo: asigned, Expected: assigned, Correction: aligned\n",
      "Typo: hanlde, Expected: handle, Correction: hands\n",
      "Typo: poining, Expected: pointing, Correction: poising\n",
      "Typo: pittsburg, Expected: pittsburgh, Correction: Pittsburgh\n",
      "Typo: inequlity, Expected: inequality, Correction: inequality\n",
      "Typo: exeption, Expected: exception, Correction: exception\n",
      "Typo: soem, Expected: some, Correction: stem\n",
      "Typo: meagniful, Expected: meaningful, Correction: manful\n",
      "Typo: securiy, Expected: security, Correction: security\n",
      "Typo: meassure, Expected: measure, Correction: reassure\n",
      "Typo: dessa, Expected: essa, Correction: Odessa\n",
      "Typo: buiild, Expected: build, Correction: build\n",
      "Typo: aproach, Expected: approach, Correction: approach\n",
      "Typo: oldeer, Expected: older, Correction: older\n",
      "Typo: aroung, Expected: around, Correction: around\n",
      "Typo: repsond, Expected: respond, Correction: resend\n",
      "Typo: explicliy, Expected: explicitly, Correction: explicit\n",
      "Typo: tranlate, Expected: translate, Correction: translate\n",
      "Typo: khói, Expected: khỏi, Correction: Thai\n",
      "Typo: embedeed, Expected: embedded, Correction: embedded\n",
      "Typo: wriitten, Expected: written, Correction: written\n",
      "Typo: defailt, Expected: default, Correction: default\n",
      "Typo: kenrel, Expected: kernel, Correction: kennel\n",
      "Typo: adventage, Expected: advantage, Correction: advantage\n",
      "Typo: validater, Expected: validator, Correction: validated\n",
      "Typo: initilise, Expected: initialize, Correction: initialise\n",
      "Typo: conise, Expected: concise, Correction: ionise\n",
      "Typo: stephan, Expected: stephen, Correction: Stephan\n",
      "Typo: persitant, Expected: persistent, Correction: persistent\n",
      "Typo: evalution, Expected: evaluation, Correction: evolution\n",
      "Typo: rysnc, Expected: rsync, Correction: sync\n",
      "Typo: githuhb, Expected: github, Correction: GitHub\n",
      "Typo: locgical, Expected: logical, Correction: logical\n",
      "Typo: rhe, Expected: the, Correction: he\n",
      "Typo: grenlets, Expected: greenlets, Correction: greets\n",
      "Typo: fultiple, Expected: multiple, Correction: multiple\n",
      "Typo: broswers, Expected: browsers, Correction: broilers\n",
      "Typo: succssful, Expected: successful, Correction: successful\n",
      "Typo: legen, Expected: legend, Correction: legmen\n",
      "Typo: unknwon, Expected: unknown, Correction: unknown\n",
      "Typo: alson, Expected: also, Correction: also\n",
      "Typo: availible, Expected: available, Correction: available\n",
      "Typo: fromm, Expected: from, Correction: Fromm\n",
      "Typo: raiils, Expected: rails, Correction: rails\n",
      "Typo: namespece, Expected: namespace, Correction: namesake\n",
      "Typo: hesistate, Expected: hesitate, Correction: hesitate\n",
      "Typo: packege, Expected: package, Correction: package\n",
      "Typo: associted, Expected: associated, Correction: associated\n",
      "Typo: provion, Expected: provision, Correction: erosion\n",
      "Typo: listenning, Expected: listening, Correction: listening\n",
      "Typo: hieght, Expected: height, Correction: bight\n",
      "Typo: developped, Expected: developed, Correction: developed\n",
      "Typo: chosed, Expected: chosen, Correction: choked\n",
      "Typo: sepcify, Expected: specify, Correction: specify\n",
      "Typo: pareuet, Expected: parquet, Correction: parquet\n",
      "Typo: maxiumum, Expected: maximum, Correction: maximum\n",
      "Typo: uma, Expected: um, Correction: um\n",
      "Typo: gutengerb, Expected: gutenberg, Correction: Gutenberg\n",
      "Typo: scss, Expected: css, Correction: sass\n",
      "Typo: candidated, Expected: candidate, Correction: candidates\n",
      "Typo: silumar, Expected: simular, Correction: sideman\n",
      "Typo: cunn, Expected: cudnn, Correction: Dunn\n",
      "Typo: sofware, Expected: software, Correction: software\n",
      "Typo: syndrom, Expected: syndrome, Correction: syndrome\n",
      "Typo: leidal, Expected: leidel, Correction: medal\n",
      "Typo: visusal, Expected: visual, Correction: visual\n",
      "Typo: pylplt, Expected: pyplot, Correction: pulpit\n",
      "Typo: appl, Expected: app, Correction: app\n",
      "Typo: versiom, Expected: version, Correction: version\n",
      "Typo: folllowing, Expected: following, Correction: following\n",
      "Typo: txsockxs, Expected: txsocks, Correction: sock's\n",
      "Typo: apllies, Expected: applies, Correction: applies\n",
      "Typo: lin, Expected: line, Correction: lit\n",
      "Typo: opperating, Expected: operating, Correction: operating\n",
      "Typo: dilligence, Expected: diligence, Correction: diligence\n",
      "Typo: joine, Expected: joined, Correction: joint\n",
      "Typo: dien, Expected: dein, Correction: die\n",
      "Typo: cahce, Expected: cache, Correction: cape\n",
      "Typo: cann, Expected: can, Correction: Mann\n",
      "Typo: redirec, Expected: redirect, Correction: redirect\n",
      "Typo: standerd, Expected: standard, Correction: stander\n",
      "Typo: numbrer, Expected: number, Correction: number\n",
      "Typo: guthub, Expected: github, Correction: gutful\n",
      "Typo: repnz, Expected: repe, Correction: rent\n",
      "Typo: incease, Expected: increase, Correction: increase\n",
      "Typo: repayed, Expected: repaid, Correction: relayed\n",
      "Typo: componets, Expected: components, Correction: components\n",
      "Typo: referennce, Expected: reference, Correction: reference\n",
      "Typo: masskew, Expected: maxskew, Correction: masker\n",
      "Typo: divder, Expected: divider, Correction: diver\n",
      "Typo: listern, Expected: listener, Correction: listen\n",
      "Typo: keword, Expected: keyword, Correction: keyword\n",
      "Typo: exeucting, Expected: executing, Correction: expecting\n",
      "Typo: hallowwing, Expected: hallowing, Correction: hallowing\n",
      "Typo: hoook, Expected: hook, Correction: hook\n",
      "Typo: rin, Expected: run, Correction: riv\n",
      "Typo: impoort, Expected: import, Correction: import\n",
      "Typo: depened, Expected: depend, Correction: deepened\n",
      "Typo: goood, Expected: good, Correction: good\n"
     ]
    }
   ],
   "source": [
    "# print typos and their corrections\n",
    "for typo, expected in zip(typos, corrections):\n",
    "    correction, _ = naiveSpellingCorrection(typo, en_dict)\n",
    "    print(f\"Typo: {typo}, Expected: {expected}, Correction: {correction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) \n",
    "\n",
    "1. Phonetic Similarity Ignored: The model does not consider phonetic similarity between words. For example, it corrected \"browers\" to \"rowers\" instead of \"browsers\". A phonetically aware correction method might have performed better for such cases.\n",
    "\n",
    "2. Lack of Contextual Understanding: The system does not take into account the context in which a word is used. This can lead to incorrect corrections where the correct word is dependent on the surrounding text. For example, \"hanlde\" was corrected to \"hands\" instead of \"handle\", which a context-aware system might have correctly identified.\n",
    "\n",
    "3. Frequency Ignorance: The correction system does not consider the frequency of word usage. More common words are more likely to be the intended correction than less common ones. For instance, \"soem\" was corrected to \"stem\" instead of the more common word \"some\". Incorporating word frequency could improve accuracy by prioritizing more likely corrections.\n",
    "\n",
    "4. Handling of Proper Nouns and Specialized Terms: The system might struggle with proper nouns (e.g., \"pittsburg\" to \"Pittsburgh\") or specialized terms, especially when such words are not present or are underrepresented in the dictionary used for corrections.\n",
    "\n",
    "5. Overfitting to Dictionary Words: Since the correction is based on the closest match within a dictionary, the system might \"overfit\" to incorrect words that are closer in edit distance but not the right correction in context. For example, \"poining\" was corrected to \"poising\" instead of \"pointing\".\n",
    "\n",
    "6. Semantic Similarity: The model does not consider the semantic similarity or the meaning of words, which can lead to choosing words that are lexically close but semantically distant from the intended correction.\n",
    "\n",
    "To address these issues, enhancements like incorporating a phonetic algorithm (e.g., Soundex or Metaphone), using context-aware models (e.g., n-gram models or neural language models like BERT for contextual prediction), and including word frequency data could significantly improve the performance and accuracy of the spelling correction system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
