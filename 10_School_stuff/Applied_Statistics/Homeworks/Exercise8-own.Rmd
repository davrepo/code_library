---
title: "Applied Statistics - Exercise 8"
output:
  pdf_document: default
  html_document: default
---

# 1. Central Limit Theorem (T)

Let $X_1, X_2,\ldots$ be a sequence of independent, identically distributed random variables, with probability density function given by $$
f(x) = \begin{cases} x, & \text{if } 0<x \leq 1 \\ -x + 2, & \text{if } 1<x<2 \\0, & \text{otherwise.} \end{cases}
$$ Use central limit theorem to approximate $P(X_1 +X_2 + \ldots + X_{277} > 301)$.

Use central limit theorem to approximate $P(X_1 +X_2 + \ldots + X_{277} > 301)$.

To approximate $P(X_1 +X_2 + \ldots + X_{277} > 301)$, we can use the central limit theorem (CLT), which states that the sum of a large number of independent and identically distributed random variables, with finite mean and variance, can be approximated by a normal distribution. Specifically, if $X_1, X_2, \ldots, X_n$ are iid random variables with mean $\mu$ and variance $\sigma^2$, then the sum $S_n = X_1 + X_2 + \ldots + X_n$ can be approximated by a normal distribution with mean $n\mu$ and variance $n\sigma^2$, as $n$ approaches infinity.

In this case, we have $X_1, X_2, \ldots, X_{277}$ are iid random variables with mean and variance given by:

$$E[X_i] = \int_{-\infty}^{\infty} xf(x)dx = \int_{0}^{1} x^2 dx + \int_{1}^{2} (-x+2)x dx = \frac{1}{3} + \frac{2}{3} = 1$$

$$E[X_i^2] = \int_{-\infty}^{\infty} x^2f(x)dx = \int_{0}^{1} x^3 dx + \int_{1}^{2} (-x+2)x^2 dx = \frac{1}{4} + \frac{11}{12} = \frac{7}{6}$$

Therefore, the variance of $X_i$ is given by:

$$Var[X_i] = E[X_i^2] - (E[X_i])^2 = \frac{7}{6} - 1^2 = \frac{1}{6}$$

By the CLT, we have:

$$\frac{S_{277} - n\mu}{\sqrt{n}\sigma} \approx N(0,1)$$

where $S_{277} = X_1 + X_2 + \ldots + X_{277}$, $\mu = E[X_i] = 1$, and $\sigma^2 = Var[X_i] = \frac{1}{6}$.

To approximate $P(X_1 +X_2 + \ldots + X_{277} > 301)$, we can standardize the random variable:

$$P\left(\frac{S_{277} - n\mu}{\sqrt{n}\sigma} > \frac{301-n\mu}{\sqrt{n}\sigma}\right) \approx P(Z > z)$$

where $Z$ is a standard normal random variable, and $z = \frac{301-n\mu}{\sqrt{n}\sigma}$.

Plugging in the values, we get:

$$z = \frac{301 - 277\cdot 1}{\sqrt{277\cdot \frac{1}{6}}} \approx 3.53$$

```{r}
# find percentage chance of greater than 3.53 standard deviations above the mean
# in a normal distribution
pnorm(3.53, mean = 0, sd = 1, lower.tail = FALSE)
```

Using a standard normal table or calculator, we can find:

$$P(Z > z) \approx 0.0002$$

Therefore, using the CLT, we can approximate $P(X_1 +X_2 + \ldots + X_{277} > 301)$ by 0.0002, or about 0.02%.

# 2. Simple Statistics (R)

Consider the `firstchi` dataset, available in the `UsingR` package, which you can load using the `library(UsingR)` statement. Using R functions, compute the following numerical statistics for the dataset.

-   the sample mean
-   the sample variance
-   the 30th empirical percentile
-   the median
-   the MAD

```{r}
# load the UsingR package
library(UsingR)
data(firstchi)
print(paste("The sample mean is", mean(firstchi)))
print(paste("The sample variance is", var(firstchi)))
print(paste("The 30th empirical percentile is", quantile(firstchi, 0.3)))
print(paste("The median is", median(firstchi)))
print(paste("The MAD is", mad(firstchi)))   # MAD = median absolute deviation from the median
```
```

You can refer to Section 2.3 of *Using R for introductory statistics*.

# 3. Recognizing plots (Theory)

Consider the following distributions:

-   $N(0,1)$
-   $N(0, 8)$
-   $N(5, 2)$
-   $Exp(2)$
-   $Exp(1/2)$

The following plots report histograms, kernel density estimates, and empirical distribution functions, each for a different dataset of 150 points generated from the above distributions. For each plot, say which type of plot it is (i.e. if it's a histogram, a kernel density estimate or an empirical distribution function), and identify from which of the above distributions it was generated.

```{r eval=FALSE, include=FALSE}
#plot Exp(2)
x <- rexp(150, 2)
hist(x, breaks = 15, freq = FALSE, main = "Exp(2)")
lines(density(x), col = "red")
# cumulative distribution
plot(ecdf(x), main = "Exp(2)")
```

```{r eval=FALSE, include=FALSE}
#plot Exp(1/2)
x <- rexp(150, 1/2)
hist(x, breaks = 15, freq = FALSE, main = "Exp(1/2)")
lines(density(x), col = "red")
plot(ecdf(x), main = "Exp(1/2)")
```

```{r eval=FALSE, include=FALSE}
# plot N(0,1) cumulative distribution
x <- rnorm(150)
plot(ecdf(x), main = "N(0,1)")
```

```{r eval=FALSE, include=FALSE}
# plot N(5,2) cumulative distribution
x <- rnorm(150, 5, 2)
plot(ecdf(x), main = "N(5,2)")
```


# 4. Plotting distributions (R)

The `diamond` dataset of the `UsingR` package contains the price in Singapore dollars of 48 diamond rings, along with their size in carats.

1.  Plot the kernel density estimate of prices. Try different bandwidths. How many modes are there? Look also at the empirical cumulative distribution function. Discuss your findings.
```{r}
# load the UsingR package
library(UsingR)
data(diamond)
# plot the kernel density estimate of prices
plot(density(diamond$price), main = "Kernel density estimate of prices")
# plot the kernel density estimate of prices with different bandwidths
plot(density(diamond$price, bw = 20), main = "Kernel density estimate of prices with bandwidth = 20")
# plot the kernel density estimate of prices with different bandwidths
plot(density(diamond$price, bw = 30), main = "Kernel density estimate of prices with bandwidth = 30")
# plot the empirical cumulative distribution function
plot(ecdf(diamond$price), main = "Empirical cumulative distribution function of prices")
```

2.  Plot a scatterplot of prices versus sizes. Does any relation between the two quantities show up?

```{r}
# plot a scatterplot of prices versus sizes
plot(diamond$carat, diamond$price, main = "Scatterplot of prices versus sizes")
# add regression line to the existing scatterplot
abline(lm(diamond$price ~ diamond$carat), col = "red")
```

# 5. Mean and median of two datasets (Theory)

Consider two datasets $x_1, \dots, x_n$ and $y_1, \dots, y_m$. Note that they have different lengths. Let $\bar{x}$ be the sample mean of the first, and $\bar{y}$ the sample mean of the second. Consider the combined dataset $x_1, \dots, x_n, y_1, \dots, y_m$ with $m + n$ elements, obtained by concatenating the two original datasets.

a.  Is it true that the sample mean of the combined dataset is equal to $\frac{\bar{x} + \bar{y}}{2}$? If yes, provide a proof, if no, provide a counterexample.



b.  Consider the case where $m = n$, i.e. the two datasets have the same size. In this special case, is the sample mean of the combined dataset equal to $\frac{\bar{x} + \bar{y}}{2}$? If yes, provide a proof, if no, provide a counterexample.
c.  Consider now the sample medians $Med_x$ and $Med_x$ of the two datasets, in the general case of $m \ne n$. Is it true that the sample median of the combined dataset is equal to $\frac{Med_x + Med_y}{2}$? If yes, provide a proof, if no, provide a counterexample.
d.  In the special case of $m = n$, is the sample median of the combined dataset is equal to $\frac{Med_x + Med_y}{2}$? If yes, provide a proof, if no, provide a counterexample.
